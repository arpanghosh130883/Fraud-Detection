{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Signupdata = pd.read_csv('/home/mvisi/Project/DLP/Core/FraudPredict/Notebook/Arpan/Signupdataimputed.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Fraud_Acc_Flag', 'EIDStatus', 'SanctionStatus', 'Ip_Latitude',\n",
       "       'Ip_Longitude', 'Ip_Carrier', 'Ip_Connection_type', 'Ip_Line_Speed',\n",
       "       'Ip_Routing_type', 'IP_Anonymizer_status', 'Fullcontact_Matched',\n",
       "       'Social_Profiles_Count', 'gender_Fullcontact', 'ageRange_Fullcontact',\n",
       "       'location_Country_Fullcontact', 'browser_online', 'brwsr_lang',\n",
       "       'brwsr_type', 'brwsr_version', 'device_manufacturer', 'device_name',\n",
       "       'device_type', 'device_os_type', 'screen_resolution', 'address_type',\n",
       "       'aza', 'country_of_residence', 'email_domain', 'region_suburb',\n",
       "       'residential_status', 'title', 'ad_campaign', 'affiliate_name',\n",
       "       'branch', 'channel', 'keywords', 'op_country', 'referral_text',\n",
       "       'reg_mode', 'search_engine', 'source', 'sub_source', 'turnover',\n",
       "       'txn_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Signupdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating Dependent and independent variables \n",
    "#data_final_Org= data_final.copy()\n",
    "Signupdata_ML = Signupdata.copy()\n",
    "X_Signupdata = Signupdata.drop(['Fraud_Acc_Flag'], axis=1, inplace=True)\n",
    "Y_Signupdata = Signupdata_ML.Fraud_Acc_Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIDStatus</th>\n",
       "      <th>SanctionStatus</th>\n",
       "      <th>Ip_Latitude</th>\n",
       "      <th>Ip_Longitude</th>\n",
       "      <th>Ip_Carrier</th>\n",
       "      <th>Ip_Connection_type</th>\n",
       "      <th>Ip_Line_Speed</th>\n",
       "      <th>Ip_Routing_type</th>\n",
       "      <th>IP_Anonymizer_status</th>\n",
       "      <th>Fullcontact_Matched</th>\n",
       "      <th>...</th>\n",
       "      <th>channel</th>\n",
       "      <th>keywords</th>\n",
       "      <th>op_country</th>\n",
       "      <th>referral_text</th>\n",
       "      <th>reg_mode</th>\n",
       "      <th>search_engine</th>\n",
       "      <th>source</th>\n",
       "      <th>sub_source</th>\n",
       "      <th>turnover</th>\n",
       "      <th>txn_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.511111</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.172000</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>0.00</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.511111</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.511111</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>276</td>\n",
       "      <td>113467.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.858334</td>\n",
       "      <td>-2.218611</td>\n",
       "      <td>6411</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>195</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EIDStatus  SanctionStatus  Ip_Latitude  Ip_Longitude  Ip_Carrier  \\\n",
       "0          0               0    51.511111     -0.040000           0   \n",
       "1          0               0    52.172000      0.116000        2010   \n",
       "2          0               0    51.511111     -0.040000           0   \n",
       "3          0               0    51.511111     -0.040000           0   \n",
       "4          0               0    51.858334     -2.218611        6411   \n",
       "\n",
       "   Ip_Connection_type  Ip_Line_Speed  Ip_Routing_type  IP_Anonymizer_status  \\\n",
       "0                   0              0                0                     0   \n",
       "1                  11              1                3                     3   \n",
       "2                   0              0                0                     0   \n",
       "3                   0              0                0                     0   \n",
       "4                   1              3                3                     3   \n",
       "\n",
       "   Fullcontact_Matched  ...  channel  keywords  op_country  referral_text  \\\n",
       "0                    1  ...        1         0           0              0   \n",
       "1                    1  ...        1         0           0              0   \n",
       "2                    0  ...        0         0           0              0   \n",
       "3                    0  ...        1         0           0              0   \n",
       "4                    1  ...        1         0           0              0   \n",
       "\n",
       "   reg_mode  search_engine  source  sub_source   turnover  txn_value  \n",
       "0         2              0      32          92       0.00         15  \n",
       "1         2              0       3         121       0.00        170  \n",
       "2         2              0       3         276       0.00          0  \n",
       "3         2              0       3         276  113467.12          0  \n",
       "4         2              0      40         195       0.00         15  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Signupdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Signupdata, Y_Signupdata, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_trainScaler = scaler.fit_transform(X_train)\n",
    "X_testScaler = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.52866513, ...,  0.69460274,\n",
       "        -0.02150941, -0.21754786],\n",
       "       [ 0.        ,  0.        ,  0.52064089, ...,  0.69460274,\n",
       "        -0.02159574, -0.21754786],\n",
       "       [ 0.        ,  0.        ,  0.55358365, ...,  0.69460274,\n",
       "        -0.02201462,  0.54931445],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.49700026, ...,  0.69460274,\n",
       "        -0.02201462, -0.851684  ],\n",
       "       [ 0.        ,  0.        ,  0.53337446, ..., -0.20875243,\n",
       "        -0.02201462,  1.74384997],\n",
       "       [ 0.        ,  0.        ,  0.49654512, ...,  1.33029712,\n",
       "        -0.02086449,  1.74384997]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_assigned={0:1,1:493}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(Signupdata.columns))\n",
    "n_inputs=43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1952.9529\n",
      "Epoch 2/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1362.8484\n",
      "Epoch 3/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1070.3486\n",
      "Epoch 4/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1003.7258\n",
      "Epoch 5/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1065.9574\n",
      "Epoch 6/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 620.1974\n",
      "Epoch 7/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 730.7646\n",
      "Epoch 8/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 513.1337\n",
      "Epoch 9/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 599.5714\n",
      "Epoch 10/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 447.2795\n",
      "Epoch 11/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 357.7430\n",
      "Epoch 12/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 331.7723\n",
      "Epoch 13/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 370.0276\n",
      "Epoch 14/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 239.1501\n",
      "Epoch 15/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 173.7342\n",
      "Epoch 16/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 255.6298\n",
      "Epoch 17/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 162.8771\n",
      "Epoch 18/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 158.7738\n",
      "Epoch 19/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 260.6867\n",
      "Epoch 20/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 130.4527\n",
      "Epoch 21/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 35.2947\n",
      "Epoch 22/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.2296\n",
      "Epoch 23/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 21.4090\n",
      "Epoch 24/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.1482\n",
      "Epoch 25/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.4455\n",
      "Epoch 26/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.9051\n",
      "Epoch 27/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.7826\n",
      "Epoch 28/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.1262\n",
      "Epoch 29/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.7719\n",
      "Epoch 30/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.7905\n",
      "Epoch 31/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.7362\n",
      "Epoch 32/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.4434\n",
      "Epoch 33/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.6276\n",
      "Epoch 34/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.9876\n",
      "Epoch 35/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.7515\n",
      "Epoch 36/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.5943\n",
      "Epoch 37/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 30.9889\n",
      "Epoch 38/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.7372\n",
      "Epoch 39/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.0200\n",
      "Epoch 40/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 24.8471\n",
      "Epoch 41/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.9362\n",
      "Epoch 42/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.0483\n",
      "Epoch 43/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 24.5398\n",
      "Epoch 44/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.0878\n",
      "Epoch 45/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.6714\n",
      "Epoch 46/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.0799\n",
      "Epoch 47/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.2307\n",
      "Epoch 48/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.6942\n",
      "Epoch 49/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.6120\n",
      "Epoch 50/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.3363\n",
      "Epoch 51/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.0856\n",
      "Epoch 52/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 22.2237\n",
      "Epoch 53/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.8647\n",
      "Epoch 54/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.7896\n",
      "Epoch 55/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.0780\n",
      "Epoch 56/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.2959\n",
      "Epoch 57/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.0276\n",
      "Epoch 58/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.1095\n",
      "Epoch 59/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 30.2037\n",
      "Epoch 60/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.8189\n",
      "Epoch 61/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 31.9473\n",
      "Epoch 62/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 41.2461\n",
      "Epoch 63/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.7487\n",
      "Epoch 64/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.8211\n",
      "Epoch 65/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.0229\n",
      "Epoch 66/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.0940\n",
      "Epoch 67/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 32.6988\n",
      "Epoch 68/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 121.7542\n",
      "Epoch 69/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 49.8877\n",
      "Epoch 70/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 31.7203\n",
      "Epoch 71/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 31.0477\n",
      "Epoch 72/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.6123\n",
      "Epoch 73/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.3550\n",
      "Epoch 74/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.1906\n",
      "Epoch 75/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.5602\n",
      "Epoch 76/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.7088\n",
      "Epoch 77/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.8778\n",
      "Epoch 78/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 25.9748\n",
      "Epoch 79/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.8188\n",
      "Epoch 80/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.8820\n",
      "Epoch 81/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.5332\n",
      "Epoch 82/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.1591\n",
      "Epoch 83/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.8970\n",
      "Epoch 84/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 28.7340\n",
      "Epoch 85/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.6728\n",
      "Epoch 86/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.7957\n",
      "Epoch 87/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.2238\n",
      "Epoch 88/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.6315\n",
      "Epoch 89/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 39.0939\n",
      "Epoch 90/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.0401\n",
      "Epoch 91/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.7744\n",
      "Epoch 92/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 21.1925\n",
      "Epoch 93/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.6006\n",
      "Epoch 94/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.7385\n",
      "Epoch 95/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.1060\n",
      "Epoch 96/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 35.4061\n",
      "Epoch 97/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 21.0869\n",
      "Epoch 98/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.3192\n",
      "Epoch 99/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.8979\n",
      "Epoch 100/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.2759\n",
      "Epoch 101/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.8887\n",
      "Epoch 102/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 21.2267\n",
      "Epoch 103/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 50.7552\n",
      "Epoch 104/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.1534\n",
      "Epoch 105/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 27.4879\n",
      "Epoch 106/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.5434\n",
      "Epoch 107/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.9864\n",
      "Epoch 108/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.7482\n",
      "Epoch 109/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 40.2223\n",
      "Epoch 110/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 24.1403\n",
      "Epoch 111/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.0015\n",
      "Epoch 112/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.5199\n",
      "Epoch 113/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.3634\n",
      "Epoch 114/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 23.8796\n",
      "Epoch 115/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.4779\n",
      "Epoch 116/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.1035\n",
      "Epoch 117/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.0599\n",
      "Epoch 118/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.5880\n",
      "Epoch 119/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.5064\n",
      "Epoch 120/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.0952\n",
      "Epoch 121/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.7493\n",
      "Epoch 122/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.9910\n",
      "Epoch 123/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 32.1098\n",
      "Epoch 124/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.9442\n",
      "Epoch 125/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.7921\n",
      "Epoch 126/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 22.7277\n",
      "Epoch 127/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.7300\n",
      "Epoch 128/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 44.6456\n",
      "Epoch 129/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.0106\n",
      "Epoch 130/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.3384\n",
      "Epoch 131/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.7470\n",
      "Epoch 132/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.3053\n",
      "Epoch 133/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.7287\n",
      "Epoch 134/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 24.7593\n",
      "Epoch 135/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.9563\n",
      "Epoch 136/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.5133\n",
      "Epoch 137/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.6915\n",
      "Epoch 138/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.4876\n",
      "Epoch 139/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 30.1840\n",
      "Epoch 140/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.9862\n",
      "Epoch 141/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.3061\n",
      "Epoch 142/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.7865\n",
      "Epoch 143/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.6178\n",
      "Epoch 144/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.7879\n",
      "Epoch 145/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 36.3484\n",
      "Epoch 146/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.3814\n",
      "Epoch 147/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.6217\n",
      "Epoch 148/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.7037\n",
      "Epoch 149/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.3603\n",
      "Epoch 150/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 28.6311\n",
      "Epoch 151/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.9614\n",
      "Epoch 152/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.3933\n",
      "Epoch 153/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.2253\n",
      "Epoch 154/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.6074\n",
      "Epoch 155/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 21.3313\n",
      "Epoch 156/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.3257\n",
      "Epoch 157/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.7844\n",
      "Epoch 158/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.4556\n",
      "Epoch 159/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.3278\n",
      "Epoch 160/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.2360\n",
      "Epoch 161/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.2468\n",
      "Epoch 162/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 23.1413\n",
      "Epoch 163/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.4500\n",
      "Epoch 164/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 36.1164\n",
      "Epoch 165/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 21.2048\n",
      "Epoch 166/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.8617\n",
      "Epoch 167/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.6034\n",
      "Epoch 168/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 49.3522\n",
      "Epoch 169/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 25.6184\n",
      "Epoch 170/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 35.8710\n",
      "Epoch 171/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 42.4229\n",
      "Epoch 172/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 31.2129\n",
      "Epoch 173/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 131.1806\n",
      "Epoch 174/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 36.3960\n",
      "Epoch 175/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.0741\n",
      "Epoch 176/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 33.6814\n",
      "Epoch 177/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 23.5639\n",
      "Epoch 178/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.4919\n",
      "Epoch 179/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 25.8767\n",
      "Epoch 180/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.1569\n",
      "Epoch 181/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.5812\n",
      "Epoch 182/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 30.0425\n",
      "Epoch 183/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 27.6760\n",
      "Epoch 184/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 46.7824\n",
      "Epoch 185/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 44.3357\n",
      "Epoch 186/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9111/9111 [==============================] - 12s 1ms/step - loss: 59.5446\n",
      "Epoch 187/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 50.6135\n",
      "Epoch 188/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 53.6108\n",
      "Epoch 189/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 37.8285\n",
      "Epoch 190/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 33.9942\n",
      "Epoch 191/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 48.6887\n",
      "Epoch 192/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 56.9957\n",
      "Epoch 193/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 23.2076\n",
      "Epoch 194/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 32.5585\n",
      "Epoch 195/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 36.6453\n",
      "Epoch 196/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.6708\n",
      "Epoch 197/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.5464\n",
      "Epoch 198/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 40.4060\n",
      "Epoch 199/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 24.2417\n",
      "Epoch 200/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 41.7363\n",
      "Epoch 201/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.7990\n",
      "Epoch 202/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 52.6840\n",
      "Epoch 203/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 104.3159\n",
      "Epoch 204/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 92.1004\n",
      "Epoch 205/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 65.3347\n",
      "Epoch 206/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 131.7076\n",
      "Epoch 207/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 82.6937\n",
      "Epoch 208/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 46.0098\n",
      "Epoch 209/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 34.5282\n",
      "Epoch 210/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 62.8046\n",
      "Epoch 211/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 37.9651\n",
      "Epoch 212/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 76.6722\n",
      "Epoch 213/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 81.3414\n",
      "Epoch 214/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 67.7356\n",
      "Epoch 215/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 50.7610\n",
      "Epoch 216/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 51.3231\n",
      "Epoch 217/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 83.7369\n",
      "Epoch 218/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 51.4390\n",
      "Epoch 219/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 83.3918\n",
      "Epoch 220/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 78.3716\n",
      "Epoch 221/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 112.0716\n",
      "Epoch 222/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 64.4553\n",
      "Epoch 223/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 81.2877\n",
      "Epoch 224/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 79.3519\n",
      "Epoch 225/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 59.3929\n",
      "Epoch 226/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 84.9488\n",
      "Epoch 227/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 96.4196\n",
      "Epoch 228/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 78.3740\n",
      "Epoch 229/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 56.3669\n",
      "Epoch 230/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 98.1267\n",
      "Epoch 231/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 70.3457\n",
      "Epoch 232/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 67.6834\n",
      "Epoch 233/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 129.5708\n",
      "Epoch 234/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 99.2592\n",
      "Epoch 235/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 64.5310\n",
      "Epoch 236/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 75.8412\n",
      "Epoch 237/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 141.7741\n",
      "Epoch 238/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 85.1607\n",
      "Epoch 239/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 134.7153\n",
      "Epoch 240/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 71.2645\n",
      "Epoch 241/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 101.9368\n",
      "Epoch 242/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 64.9328\n",
      "Epoch 243/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 64.8907\n",
      "Epoch 244/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 29.3566\n",
      "Epoch 245/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 23.6366\n",
      "Epoch 246/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 28.6145\n",
      "Epoch 247/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 33.3578\n",
      "Epoch 248/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 36.0422\n",
      "Epoch 249/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 50.2617\n",
      "Epoch 250/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 51.4178\n",
      "Epoch 251/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 83.9025\n",
      "Epoch 252/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 63.1824\n",
      "Epoch 253/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 64.8622\n",
      "Epoch 254/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 57.5381\n",
      "Epoch 255/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 60.4318\n",
      "Epoch 256/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 38.7038\n",
      "Epoch 257/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 31.6927\n",
      "Epoch 258/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 149.4443\n",
      "Epoch 259/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 122.1046\n",
      "Epoch 260/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 70.7406\n",
      "Epoch 261/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 77.4076\n",
      "Epoch 262/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 51.6184\n",
      "Epoch 263/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 87.6018\n",
      "Epoch 264/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 53.4712\n",
      "Epoch 265/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 112.8998\n",
      "Epoch 266/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 97.0379\n",
      "Epoch 267/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 45.0474\n",
      "Epoch 268/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 92.7455\n",
      "Epoch 269/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 40.3450\n",
      "Epoch 270/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.2497\n",
      "Epoch 271/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 28.0370\n",
      "Epoch 272/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 34.1666\n",
      "Epoch 273/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 25.6989\n",
      "Epoch 274/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 39.4827\n",
      "Epoch 275/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 24.0303\n",
      "Epoch 276/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 35.5587\n",
      "Epoch 277/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 33.3254\n",
      "Epoch 278/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.9066\n",
      "Epoch 279/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 33.1028\n",
      "Epoch 280/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 30.2634\n",
      "Epoch 281/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 28.2057\n",
      "Epoch 282/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.4556\n",
      "Epoch 283/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 23.1867\n",
      "Epoch 284/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.1078\n",
      "Epoch 285/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.7417\n",
      "Epoch 286/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.9851\n",
      "Epoch 287/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 24.4885\n",
      "Epoch 288/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.8937\n",
      "Epoch 289/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 22.2074\n",
      "Epoch 290/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 34.8283\n",
      "Epoch 291/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 23.1224\n",
      "Epoch 292/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.0560\n",
      "Epoch 293/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.2532\n",
      "Epoch 294/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.9470\n",
      "Epoch 295/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 27.5115\n",
      "Epoch 296/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.6742\n",
      "Epoch 297/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 23.4183\n",
      "Epoch 298/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.0376\n",
      "Epoch 299/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.9919\n",
      "Epoch 300/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.5440\n",
      "Epoch 301/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.8690\n",
      "Epoch 302/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.1254\n",
      "Epoch 303/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.1683\n",
      "Epoch 304/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.9853\n",
      "Epoch 305/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.2423\n",
      "Epoch 306/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 28.2152\n",
      "Epoch 307/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 27.5637\n",
      "Epoch 308/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.4579\n",
      "Epoch 309/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.7814\n",
      "Epoch 310/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.5397\n",
      "Epoch 311/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.5068\n",
      "Epoch 312/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.7397\n",
      "Epoch 313/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.0532\n",
      "Epoch 314/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.3817\n",
      "Epoch 315/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 29.0441\n",
      "Epoch 316/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.6427\n",
      "Epoch 317/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.2260\n",
      "Epoch 318/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.1646\n",
      "Epoch 319/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 21.9606\n",
      "Epoch 320/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.3179\n",
      "Epoch 321/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.5725\n",
      "Epoch 322/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.9798\n",
      "Epoch 323/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 52.8889\n",
      "Epoch 324/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 22.4031\n",
      "Epoch 325/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.8384\n",
      "Epoch 326/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.0884\n",
      "Epoch 327/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.8643\n",
      "Epoch 328/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 22.1752\n",
      "Epoch 329/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.8083\n",
      "Epoch 330/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.1861\n",
      "Epoch 331/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.3193\n",
      "Epoch 332/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.1041\n",
      "Epoch 333/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 25.4156\n",
      "Epoch 334/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 39.7740\n",
      "Epoch 335/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.8149\n",
      "Epoch 336/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.9633\n",
      "Epoch 337/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.2362\n",
      "Epoch 338/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 31.0854\n",
      "Epoch 339/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 122.2745\n",
      "Epoch 340/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 29.4378\n",
      "Epoch 341/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.2878\n",
      "Epoch 342/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.1842\n",
      "Epoch 343/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.3990\n",
      "Epoch 344/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.2527\n",
      "Epoch 345/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.2525\n",
      "Epoch 346/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.4647\n",
      "Epoch 347/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 44.5232\n",
      "Epoch 348/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.4900\n",
      "Epoch 349/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 29.8993\n",
      "Epoch 350/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 22.1650\n",
      "Epoch 351/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.9912\n",
      "Epoch 352/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 41.5051\n",
      "Epoch 353/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 45.1397\n",
      "Epoch 354/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 38.3738\n",
      "Epoch 355/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.6680\n",
      "Epoch 356/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.4535\n",
      "Epoch 357/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 37.5471\n",
      "Epoch 358/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 25.0596\n",
      "Epoch 359/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.1498\n",
      "Epoch 360/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.6105\n",
      "Epoch 361/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.8369\n",
      "Epoch 362/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 21.6596\n",
      "Epoch 363/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.2297\n",
      "Epoch 364/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.0937\n",
      "Epoch 365/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.1938\n",
      "Epoch 366/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 30.1327\n",
      "Epoch 367/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.5010\n",
      "Epoch 368/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.1105\n",
      "Epoch 369/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.9617\n",
      "Epoch 370/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.4282\n",
      "Epoch 371/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.6045\n",
      "Epoch 372/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 25.2310\n",
      "Epoch 373/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.8661\n",
      "Epoch 374/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.5760\n",
      "Epoch 375/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.8071\n",
      "Epoch 376/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.3903\n",
      "Epoch 377/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.3398\n",
      "Epoch 378/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.9298\n",
      "Epoch 379/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.1987\n",
      "Epoch 380/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.9034\n",
      "Epoch 381/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.7920\n",
      "Epoch 382/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.0425\n",
      "Epoch 383/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.0677\n",
      "Epoch 384/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.4449\n",
      "Epoch 385/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.8717\n",
      "Epoch 386/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.1992\n",
      "Epoch 387/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.8488\n",
      "Epoch 388/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.5298\n",
      "Epoch 389/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.5395\n",
      "Epoch 390/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.3192\n",
      "Epoch 391/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.7235\n",
      "Epoch 392/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.4956\n",
      "Epoch 393/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.7604\n",
      "Epoch 394/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6727\n",
      "Epoch 395/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.2454\n",
      "Epoch 396/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.4279\n",
      "Epoch 397/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5890\n",
      "Epoch 398/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7189\n",
      "Epoch 399/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.2002\n",
      "Epoch 400/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.7649\n",
      "Epoch 401/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0458\n",
      "Epoch 402/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7901\n",
      "Epoch 403/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3194\n",
      "Epoch 404/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6294\n",
      "Epoch 405/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7040\n",
      "Epoch 406/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5745\n",
      "Epoch 407/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7272\n",
      "Epoch 408/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5239\n",
      "Epoch 409/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5199\n",
      "Epoch 410/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4411\n",
      "Epoch 411/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.1997\n",
      "Epoch 412/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.7491\n",
      "Epoch 413/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4553\n",
      "Epoch 414/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.2618\n",
      "Epoch 415/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.6281\n",
      "Epoch 416/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.0598\n",
      "Epoch 417/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.7065\n",
      "Epoch 418/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3085\n",
      "Epoch 419/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.6922\n",
      "Epoch 420/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.5737\n",
      "Epoch 421/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.0120\n",
      "Epoch 422/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3516\n",
      "Epoch 423/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.4223\n",
      "Epoch 424/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0706\n",
      "Epoch 425/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.4135\n",
      "Epoch 426/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.6905\n",
      "Epoch 427/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8408\n",
      "Epoch 428/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4075\n",
      "Epoch 429/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8950\n",
      "Epoch 430/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.7410\n",
      "Epoch 431/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8459\n",
      "Epoch 432/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.7027\n",
      "Epoch 433/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0051\n",
      "Epoch 434/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.2220\n",
      "Epoch 435/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.1613\n",
      "Epoch 436/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.0859\n",
      "Epoch 437/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.5434\n",
      "Epoch 438/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4799\n",
      "Epoch 439/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.1687\n",
      "Epoch 440/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.5534\n",
      "Epoch 441/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.7177\n",
      "Epoch 442/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8878\n",
      "Epoch 443/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8324\n",
      "Epoch 444/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.7304\n",
      "Epoch 445/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9702\n",
      "Epoch 446/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.2041\n",
      "Epoch 447/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.1307\n",
      "Epoch 448/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9480\n",
      "Epoch 449/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.2529\n",
      "Epoch 450/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5660\n",
      "Epoch 451/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.8277\n",
      "Epoch 452/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.3459\n",
      "Epoch 453/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.9504\n",
      "Epoch 454/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5420\n",
      "Epoch 455/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.7487\n",
      "Epoch 456/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.0588\n",
      "Epoch 457/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.9537\n",
      "Epoch 458/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.4680\n",
      "Epoch 459/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.0946\n",
      "Epoch 460/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.2885\n",
      "Epoch 461/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.9952\n",
      "Epoch 462/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.5306\n",
      "Epoch 463/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9111/9111 [==============================] - 12s 1ms/step - loss: 32.1346\n",
      "Epoch 464/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.9311\n",
      "Epoch 465/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.8599\n",
      "Epoch 466/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.0021\n",
      "Epoch 467/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 22.5797\n",
      "Epoch 468/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.7426\n",
      "Epoch 469/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.1841\n",
      "Epoch 470/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 51.9577\n",
      "Epoch 471/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.7700\n",
      "Epoch 472/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.5782\n",
      "Epoch 473/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 46.7094\n",
      "Epoch 474/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.0990\n",
      "Epoch 475/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 29.6787\n",
      "Epoch 476/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.9119\n",
      "Epoch 477/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.5836\n",
      "Epoch 478/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.5823\n",
      "Epoch 479/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.9024\n",
      "Epoch 480/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.5115\n",
      "Epoch 481/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.8418\n",
      "Epoch 482/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.5560\n",
      "Epoch 483/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.5513\n",
      "Epoch 484/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 33.5478\n",
      "Epoch 485/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.3015\n",
      "Epoch 486/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.0792\n",
      "Epoch 487/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.4549\n",
      "Epoch 488/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.6690\n",
      "Epoch 489/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.9237\n",
      "Epoch 490/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.9024\n",
      "Epoch 491/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.2028\n",
      "Epoch 492/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.1259\n",
      "Epoch 493/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.2012\n",
      "Epoch 494/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.4420\n",
      "Epoch 495/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.8823\n",
      "Epoch 496/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.0882\n",
      "Epoch 497/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.0016\n",
      "Epoch 498/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.0143\n",
      "Epoch 499/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.9517\n",
      "Epoch 500/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.0781\n",
      "Epoch 501/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 21.7947\n",
      "Epoch 502/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.3001\n",
      "Epoch 503/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.2421\n",
      "Epoch 504/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.7038\n",
      "Epoch 505/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.9092\n",
      "Epoch 506/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 27.6440\n",
      "Epoch 507/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 34.4565\n",
      "Epoch 508/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.7303\n",
      "Epoch 509/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.0543\n",
      "Epoch 510/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.2018\n",
      "Epoch 511/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.7551\n",
      "Epoch 512/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.3016\n",
      "Epoch 513/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 22.0732\n",
      "Epoch 514/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 24.3438\n",
      "Epoch 515/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.8524\n",
      "Epoch 516/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.1591\n",
      "Epoch 517/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 31.0586\n",
      "Epoch 518/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.0142\n",
      "Epoch 519/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.4157\n",
      "Epoch 520/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.7589\n",
      "Epoch 521/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.3134\n",
      "Epoch 522/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.4035\n",
      "Epoch 523/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.2739\n",
      "Epoch 524/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.2965\n",
      "Epoch 525/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.0248\n",
      "Epoch 526/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 32.3081\n",
      "Epoch 527/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.1703\n",
      "Epoch 528/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.9337\n",
      "Epoch 529/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.9375\n",
      "Epoch 530/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.2218\n",
      "Epoch 531/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.3561\n",
      "Epoch 532/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.2721\n",
      "Epoch 533/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.5476\n",
      "Epoch 534/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7965\n",
      "Epoch 535/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.9008\n",
      "Epoch 536/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.2460\n",
      "Epoch 537/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.6086\n",
      "Epoch 538/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.2069\n",
      "Epoch 539/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4724\n",
      "Epoch 540/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.4351\n",
      "Epoch 541/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.2712\n",
      "Epoch 542/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.1974\n",
      "Epoch 543/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.3828\n",
      "Epoch 544/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9303\n",
      "Epoch 545/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.3098\n",
      "Epoch 546/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.2274\n",
      "Epoch 547/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.8941\n",
      "Epoch 548/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5740\n",
      "Epoch 549/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.1615\n",
      "Epoch 550/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.0454\n",
      "Epoch 551/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.7957\n",
      "Epoch 552/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8575\n",
      "Epoch 553/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.6264\n",
      "Epoch 554/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.6818\n",
      "Epoch 555/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.2354\n",
      "Epoch 556/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3717\n",
      "Epoch 557/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9642\n",
      "Epoch 558/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.1597\n",
      "Epoch 559/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.3312\n",
      "Epoch 560/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.3013\n",
      "Epoch 561/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.6261\n",
      "Epoch 562/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.4077\n",
      "Epoch 563/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.5792\n",
      "Epoch 564/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7660\n",
      "Epoch 565/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3847\n",
      "Epoch 566/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.9759\n",
      "Epoch 567/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.0142\n",
      "Epoch 568/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4397\n",
      "Epoch 569/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3838\n",
      "Epoch 570/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.8673\n",
      "Epoch 571/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7082\n",
      "Epoch 572/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4441\n",
      "Epoch 573/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3827\n",
      "Epoch 574/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9897\n",
      "Epoch 575/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8341\n",
      "Epoch 576/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.1116\n",
      "Epoch 577/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.2533\n",
      "Epoch 578/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7211\n",
      "Epoch 579/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.5892\n",
      "Epoch 580/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5513\n",
      "Epoch 581/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.4944\n",
      "Epoch 582/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.2557\n",
      "Epoch 583/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.9854\n",
      "Epoch 584/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.4324\n",
      "Epoch 585/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.2768\n",
      "Epoch 586/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5360\n",
      "Epoch 587/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.7893\n",
      "Epoch 588/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6323\n",
      "Epoch 589/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5021\n",
      "Epoch 590/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.7192\n",
      "Epoch 591/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.4709\n",
      "Epoch 592/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.7382\n",
      "Epoch 593/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8130\n",
      "Epoch 594/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.0990\n",
      "Epoch 595/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.9389\n",
      "Epoch 596/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5256\n",
      "Epoch 597/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5160\n",
      "Epoch 598/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5303\n",
      "Epoch 599/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.2597\n",
      "Epoch 600/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.4001\n",
      "Epoch 601/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.0461\n",
      "Epoch 602/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.0918\n",
      "Epoch 603/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.6915\n",
      "Epoch 604/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.4643\n",
      "Epoch 605/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 29.7145\n",
      "Epoch 606/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.4226\n",
      "Epoch 607/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.7900\n",
      "Epoch 608/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.6021\n",
      "Epoch 609/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4815\n",
      "Epoch 610/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.7967\n",
      "Epoch 611/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.4554\n",
      "Epoch 612/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4078\n",
      "Epoch 613/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9422\n",
      "Epoch 614/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5161\n",
      "Epoch 615/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.7757\n",
      "Epoch 616/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.0153\n",
      "Epoch 617/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.2078\n",
      "Epoch 618/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.4019\n",
      "Epoch 619/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.4989\n",
      "Epoch 620/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8509\n",
      "Epoch 621/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3869\n",
      "Epoch 622/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9652\n",
      "Epoch 623/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8600\n",
      "Epoch 624/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3917\n",
      "Epoch 625/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.4166\n",
      "Epoch 626/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.8211\n",
      "Epoch 627/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.3453\n",
      "Epoch 628/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.1841\n",
      "Epoch 629/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.0582\n",
      "Epoch 630/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.2201\n",
      "Epoch 631/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.3305\n",
      "Epoch 632/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0652\n",
      "Epoch 633/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.4515\n",
      "Epoch 634/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8478\n",
      "Epoch 635/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3840\n",
      "Epoch 636/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.1012\n",
      "Epoch 637/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9818\n",
      "Epoch 638/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4260\n",
      "Epoch 639/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7270\n",
      "Epoch 640/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5069\n",
      "Epoch 641/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.9163\n",
      "Epoch 642/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.4076\n",
      "Epoch 643/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.5293\n",
      "Epoch 644/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.4075\n",
      "Epoch 645/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.1178\n",
      "Epoch 646/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5705\n",
      "Epoch 647/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8581\n",
      "Epoch 648/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9234\n",
      "Epoch 649/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.3418\n",
      "Epoch 650/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.8200\n",
      "Epoch 651/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.4316\n",
      "Epoch 652/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.0249\n",
      "Epoch 653/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7573\n",
      "Epoch 654/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.5149\n",
      "Epoch 655/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.1970\n",
      "Epoch 656/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.3812\n",
      "Epoch 657/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6848\n",
      "Epoch 658/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3880\n",
      "Epoch 659/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.4465\n",
      "Epoch 660/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8217\n",
      "Epoch 661/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4852\n",
      "Epoch 662/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5213\n",
      "Epoch 663/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6539\n",
      "Epoch 664/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4996\n",
      "Epoch 665/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3883\n",
      "Epoch 666/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5150\n",
      "Epoch 667/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3884\n",
      "Epoch 668/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6078\n",
      "Epoch 669/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4746\n",
      "Epoch 670/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7126\n",
      "Epoch 671/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6444\n",
      "Epoch 672/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4900\n",
      "Epoch 673/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.5845\n",
      "Epoch 674/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8133\n",
      "Epoch 675/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.4761\n",
      "Epoch 676/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.7236\n",
      "Epoch 677/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.3731\n",
      "Epoch 678/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5183\n",
      "Epoch 679/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8736\n",
      "Epoch 680/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3999\n",
      "Epoch 681/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.1341\n",
      "Epoch 682/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5877\n",
      "Epoch 683/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4222\n",
      "Epoch 684/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3948\n",
      "Epoch 685/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5416\n",
      "Epoch 686/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.4866\n",
      "Epoch 687/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.4532\n",
      "Epoch 688/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3266\n",
      "Epoch 689/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5576\n",
      "Epoch 690/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3953\n",
      "Epoch 691/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8267\n",
      "Epoch 692/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5415\n",
      "Epoch 693/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7471\n",
      "Epoch 694/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0672\n",
      "Epoch 695/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.6602\n",
      "Epoch 696/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.1981\n",
      "Epoch 697/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8941\n",
      "Epoch 698/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.8562\n",
      "Epoch 699/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.5548\n",
      "Epoch 700/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4880\n",
      "Epoch 701/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.0616\n",
      "Epoch 702/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8784\n",
      "Epoch 703/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9078\n",
      "Epoch 704/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4477\n",
      "Epoch 705/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9582\n",
      "Epoch 706/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.4886\n",
      "Epoch 707/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8546\n",
      "Epoch 708/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5943\n",
      "Epoch 709/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6262\n",
      "Epoch 710/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8653\n",
      "Epoch 711/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.9594\n",
      "Epoch 712/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5785\n",
      "Epoch 713/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8117\n",
      "Epoch 714/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.0300\n",
      "Epoch 715/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7492\n",
      "Epoch 716/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6974\n",
      "Epoch 717/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.9207\n",
      "Epoch 718/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8454\n",
      "Epoch 719/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3596\n",
      "Epoch 720/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.9881\n",
      "Epoch 721/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5765\n",
      "Epoch 722/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0735\n",
      "Epoch 723/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.4769\n",
      "Epoch 724/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3571\n",
      "Epoch 725/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6566\n",
      "Epoch 726/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.4594\n",
      "Epoch 727/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.1156\n",
      "Epoch 728/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.6300\n",
      "Epoch 729/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.1594\n",
      "Epoch 730/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.2421\n",
      "Epoch 731/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3426\n",
      "Epoch 732/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.1211\n",
      "Epoch 733/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3514\n",
      "Epoch 734/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.6377\n",
      "Epoch 735/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4237\n",
      "Epoch 736/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8397\n",
      "Epoch 737/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.3178\n",
      "Epoch 738/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0569\n",
      "Epoch 739/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5991\n",
      "Epoch 740/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9449\n",
      "Epoch 741/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.5251\n",
      "Epoch 742/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8331\n",
      "Epoch 743/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5505\n",
      "Epoch 744/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6319\n",
      "Epoch 745/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5764\n",
      "Epoch 746/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.8109\n",
      "Epoch 747/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.1831\n",
      "Epoch 748/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.6360\n",
      "Epoch 749/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.6145\n",
      "Epoch 750/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.0310\n",
      "Epoch 751/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.9969\n",
      "Epoch 752/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.2697\n",
      "Epoch 753/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.5760\n",
      "Epoch 754/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8550\n",
      "Epoch 755/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4121\n",
      "Epoch 756/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.9609\n",
      "Epoch 757/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.5545\n",
      "Epoch 758/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7118\n",
      "Epoch 759/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3891\n",
      "Epoch 760/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.6253\n",
      "Epoch 761/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.2585\n",
      "Epoch 762/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5008\n",
      "Epoch 763/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.0047\n",
      "Epoch 764/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.8850\n",
      "Epoch 765/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.6792\n",
      "Epoch 766/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7586\n",
      "Epoch 767/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.1411\n",
      "Epoch 768/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.6898\n",
      "Epoch 769/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3243\n",
      "Epoch 770/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.4807\n",
      "Epoch 771/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.5340\n",
      "Epoch 772/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.3708\n",
      "Epoch 773/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0953\n",
      "Epoch 774/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.0655\n",
      "Epoch 775/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8276\n",
      "Epoch 776/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5233\n",
      "Epoch 777/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6471\n",
      "Epoch 778/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.9297\n",
      "Epoch 779/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7213\n",
      "Epoch 780/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.9249\n",
      "Epoch 781/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7356\n",
      "Epoch 782/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.1406\n",
      "Epoch 783/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.6370\n",
      "Epoch 784/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9971\n",
      "Epoch 785/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.7001\n",
      "Epoch 786/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.1580\n",
      "Epoch 787/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3807\n",
      "Epoch 788/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6218\n",
      "Epoch 789/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.2742\n",
      "Epoch 790/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.3648\n",
      "Epoch 791/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.5959\n",
      "Epoch 792/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.2037\n",
      "Epoch 793/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.7359\n",
      "Epoch 794/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8153\n",
      "Epoch 795/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.5263\n",
      "Epoch 796/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.5394\n",
      "Epoch 797/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.5279\n",
      "Epoch 798/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.4701\n",
      "Epoch 799/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0627\n",
      "Epoch 800/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5489\n",
      "Epoch 801/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3864\n",
      "Epoch 802/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8501\n",
      "Epoch 803/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.3617\n",
      "Epoch 804/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0957\n",
      "Epoch 805/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.7646\n",
      "Epoch 806/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.4191\n",
      "Epoch 807/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6792\n",
      "Epoch 808/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.4903\n",
      "Epoch 809/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.7999\n",
      "Epoch 810/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.8762\n",
      "Epoch 811/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6871\n",
      "Epoch 812/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5718\n",
      "Epoch 813/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.2958\n",
      "Epoch 814/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.8626\n",
      "Epoch 815/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6576\n",
      "Epoch 816/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.2382\n",
      "Epoch 817/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.9598\n",
      "Epoch 818/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.5115\n",
      "Epoch 819/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.3367\n",
      "Epoch 820/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.8474\n",
      "Epoch 821/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.8451\n",
      "Epoch 822/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.6146\n",
      "Epoch 823/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6952\n",
      "Epoch 824/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8235\n",
      "Epoch 825/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.6378\n",
      "Epoch 826/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.3677\n",
      "Epoch 827/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.8659\n",
      "Epoch 828/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.7793\n",
      "Epoch 829/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.4089\n",
      "Epoch 830/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.6803\n",
      "Epoch 831/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.1330\n",
      "Epoch 832/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5515\n",
      "Epoch 833/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0142\n",
      "Epoch 834/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.2993\n",
      "Epoch 835/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 24.1412\n",
      "Epoch 836/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 27.5846\n",
      "Epoch 837/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.0467\n",
      "Epoch 838/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 41.6505\n",
      "Epoch 839/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 37.2306\n",
      "Epoch 840/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 97.4675\n",
      "Epoch 841/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 28.8006\n",
      "Epoch 842/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.0809\n",
      "Epoch 843/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.1173\n",
      "Epoch 844/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 23.5307\n",
      "Epoch 845/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.7696\n",
      "Epoch 846/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.5820\n",
      "Epoch 847/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.6211\n",
      "Epoch 848/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.2249\n",
      "Epoch 849/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.0568\n",
      "Epoch 850/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.5335\n",
      "Epoch 851/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.9707\n",
      "Epoch 852/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.6986\n",
      "Epoch 853/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 28.3859\n",
      "Epoch 854/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.4264\n",
      "Epoch 855/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 41.9072\n",
      "Epoch 856/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 20.3140\n",
      "Epoch 857/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 28.7107\n",
      "Epoch 858/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 46.4346\n",
      "Epoch 859/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 36.0286\n",
      "Epoch 860/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 31.6058\n",
      "Epoch 861/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 28.0264\n",
      "Epoch 862/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.5067\n",
      "Epoch 863/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 23.5186\n",
      "Epoch 864/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.5782\n",
      "Epoch 865/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 33.1921\n",
      "Epoch 866/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.9535\n",
      "Epoch 867/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.6366\n",
      "Epoch 868/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.4427\n",
      "Epoch 869/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.2174\n",
      "Epoch 870/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.0032\n",
      "Epoch 871/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.9267\n",
      "Epoch 872/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.7887\n",
      "Epoch 873/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.7648\n",
      "Epoch 874/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.5228\n",
      "Epoch 875/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 24.0715\n",
      "Epoch 876/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.9383\n",
      "Epoch 877/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.7855\n",
      "Epoch 878/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.6871\n",
      "Epoch 879/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.7535\n",
      "Epoch 880/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.2948\n",
      "Epoch 881/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.1029\n",
      "Epoch 882/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 33.1015\n",
      "Epoch 883/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.7305\n",
      "Epoch 884/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 91.3640\n",
      "Epoch 885/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 62.2415\n",
      "Epoch 886/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 50.3653\n",
      "Epoch 887/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 34.5535\n",
      "Epoch 888/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.4729\n",
      "Epoch 889/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 42.3335\n",
      "Epoch 890/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.1962\n",
      "Epoch 891/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 37.6498\n",
      "Epoch 892/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 22.5485\n",
      "Epoch 893/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.0053\n",
      "Epoch 894/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 35.6724\n",
      "Epoch 895/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.8129\n",
      "Epoch 896/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.7662\n",
      "Epoch 897/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.0634\n",
      "Epoch 898/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.7088\n",
      "Epoch 899/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.4363\n",
      "Epoch 900/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.6553\n",
      "Epoch 901/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.0990\n",
      "Epoch 902/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.2686\n",
      "Epoch 903/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.6609\n",
      "Epoch 904/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.7685\n",
      "Epoch 905/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.1093\n",
      "Epoch 906/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.0373\n",
      "Epoch 907/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.4149\n",
      "Epoch 908/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.5622\n",
      "Epoch 909/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 33.3302\n",
      "Epoch 910/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.9488\n",
      "Epoch 911/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.7561\n",
      "Epoch 912/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.8184\n",
      "Epoch 913/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.3957\n",
      "Epoch 914/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.7992\n",
      "Epoch 915/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.4769\n",
      "Epoch 916/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.0933\n",
      "Epoch 917/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.9291\n",
      "Epoch 918/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.5249\n",
      "Epoch 919/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.7527\n",
      "Epoch 920/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 35.7341\n",
      "Epoch 921/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 37.8188\n",
      "Epoch 922/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 38.8125\n",
      "Epoch 923/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 40.1844\n",
      "Epoch 924/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 33.2695\n",
      "Epoch 925/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.1537\n",
      "Epoch 926/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 18.8765\n",
      "Epoch 927/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 21.0875\n",
      "Epoch 928/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 35.4969\n",
      "Epoch 929/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.6397\n",
      "Epoch 930/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.8812\n",
      "Epoch 931/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.9622\n",
      "Epoch 932/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.8298\n",
      "Epoch 933/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.2166\n",
      "Epoch 934/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 26.1507\n",
      "Epoch 935/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 36.0545\n",
      "Epoch 936/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.3999\n",
      "Epoch 937/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 22.3144\n",
      "Epoch 938/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 33.0460\n",
      "Epoch 939/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 40.0926\n",
      "Epoch 940/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.5696\n",
      "Epoch 941/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.7077\n",
      "Epoch 942/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 47.5813\n",
      "Epoch 943/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.7816\n",
      "Epoch 944/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 29.0114\n",
      "Epoch 945/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.7640\n",
      "Epoch 946/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 16.9432\n",
      "Epoch 947/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.4826\n",
      "Epoch 948/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.9959\n",
      "Epoch 949/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 13.6030\n",
      "Epoch 950/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.0531\n",
      "Epoch 951/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.3169\n",
      "Epoch 952/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.0979\n",
      "Epoch 953/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.0144\n",
      "Epoch 954/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 23.8991\n",
      "Epoch 955/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.2808\n",
      "Epoch 956/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 22.0485\n",
      "Epoch 957/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.2561\n",
      "Epoch 958/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 17.1645\n",
      "Epoch 959/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.3132\n",
      "Epoch 960/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.0827\n",
      "Epoch 961/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 8.2971\n",
      "Epoch 962/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.8143\n",
      "Epoch 963/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 11.8030\n",
      "Epoch 964/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.9856\n",
      "Epoch 965/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.6360\n",
      "Epoch 966/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 15.1857\n",
      "Epoch 967/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.2808\n",
      "Epoch 968/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 19.2486\n",
      "Epoch 969/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 10.3228\n",
      "Epoch 970/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 6.9781\n",
      "Epoch 971/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.0455\n",
      "Epoch 972/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.9114\n",
      "Epoch 973/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.9734\n",
      "Epoch 974/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.5665\n",
      "Epoch 975/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.7734\n",
      "Epoch 976/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.8110\n",
      "Epoch 977/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.1016\n",
      "Epoch 978/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.5917\n",
      "Epoch 979/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.8524\n",
      "Epoch 980/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.4823\n",
      "Epoch 981/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.0987\n",
      "Epoch 982/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 7.2983\n",
      "Epoch 983/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.6011\n",
      "Epoch 984/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 2.0784\n",
      "Epoch 985/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.4778\n",
      "Epoch 986/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 5.3665\n",
      "Epoch 987/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.3905\n",
      "Epoch 988/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.8453\n",
      "Epoch 989/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.3694\n",
      "Epoch 990/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.0819\n",
      "Epoch 991/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.9145\n",
      "Epoch 992/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 14.1528\n",
      "Epoch 993/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.7057\n",
      "Epoch 994/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.5607\n",
      "Epoch 995/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 9.2105\n",
      "Epoch 996/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 4.0941\n",
      "Epoch 997/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6215\n",
      "Epoch 998/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 12.7173\n",
      "Epoch 999/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 3.5541\n",
      "Epoch 1000/1000\n",
      "9111/9111 [==============================] - 12s 1ms/step - loss: 1.6795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad85b250f0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "# define first hidden layer and visible layer\n",
    "model.add(Dense(50, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# define loss and optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.fit(X_train,y_train,class_weight=weights_assigned, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5076136872330685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred1=model.predict(X_test)\n",
    "print(roc_auc_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5167987],\n",
       "       [0.5167987],\n",
       "       [0.5167987],\n",
       "       ...,\n",
       "       [0.5167987],\n",
       "       [0.5167987],\n",
       "       [0.5167987]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2956 121755]\n",
      " [     2    235]]\n",
      "3191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.05    124711\n",
      "           1       0.00      0.99      0.00       237\n",
      "\n",
      "    accuracy                           0.03    124948\n",
      "   macro avg       0.50      0.51      0.03    124948\n",
      "weighted avg       1.00      0.03      0.05    124948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, accuracy_score,classification_report\n",
    "print(confusion_matrix(y_test, y_pred1.round(), normalize=None))\n",
    "print(accuracy_score(y_test, y_pred1.round(), normalize=False))\n",
    "print(classification_report(y_test, y_pred1.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting keras-tuner\n",
      "  Downloading keras-tuner-1.0.2.tar.gz (62 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62 kB 9.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (20.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/mvisi/.local/lib/python3.6/site-packages (from keras-tuner) (1.19.5)\n",
      "Requirement already satisfied: tabulate in /home/mvisi/.local/lib/python3.6/site-packages (from keras-tuner) (0.8.9)\n",
      "Collecting terminaltables\n",
      "  Downloading terminaltables-3.1.0.tar.gz (12 kB)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from keras-tuner) (0.3.7)\n",
      "Requirement already satisfied: tqdm in /home/mvisi/.local/lib/python3.6/site-packages (from keras-tuner) (4.58.0)\n",
      "Requirement already satisfied: requests in /home/mvisi/.local/lib/python3.6/site-packages (from keras-tuner) (2.25.1)\n",
      "Requirement already satisfied: scipy in /home/mvisi/.local/lib/python3.6/site-packages (from keras-tuner) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /home/mvisi/.local/lib/python3.6/site-packages (from keras-tuner) (0.23.2)\n",
      "Requirement already satisfied: six in /home/mvisi/.local/lib/python3.6/site-packages (from packaging->keras-tuner) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner) (2.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->keras-tuner) (2018.1.18)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mvisi/.local/lib/python3.6/site-packages (from requests->keras-tuner) (1.24.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/mvisi/.local/lib/python3.6/site-packages (from requests->keras-tuner) (2.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/mvisi/.local/lib/python3.6/site-packages (from scikit-learn->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/mvisi/.local/lib/python3.6/site-packages (from scikit-learn->keras-tuner) (0.14.1)\n",
      "Building wheels for collected packages: keras-tuner, terminaltables\n",
      "  Building wheel for keras-tuner (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-py3-none-any.whl size=78937 sha256=780002b51f3ce0d4962fe8a324f9303f05ef5a4edab0dda2c609409f0ab71412\n",
      "  Stored in directory: /home/mvisi/.cache/pip/wheels/f9/42/e3/73f763092b16b23350dbc5b7d9b6220bdbff2944ffcc2c612b\n",
      "  Building wheel for terminaltables (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for terminaltables: filename=terminaltables-3.1.0-py3-none-any.whl size=15354 sha256=342892334156f31f86aac645543009dfbab412bafdac3e073271ec1c27ab648a\n",
      "  Stored in directory: /home/mvisi/.cache/pip/wheels/86/1b/58/c23af2fe683acd8edc15d5a1268f0242be1ff2cf827fe34737\n",
      "Successfully built keras-tuner terminaltables\n",
      "Installing collected packages: terminaltables, keras-tuner\n",
      "Successfully installed keras-tuner-1.0.2 terminaltables-3.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int('num_layers', 2, 20)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=32,\n",
    "                                            max_value=512,\n",
    "                                            step=32),\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    #objective=kt.Objective(\"val_recall\", direction=\"max\"),#'val_accuracy',\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='Fraud',\n",
    "    project_name='FraudSignup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 5\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 20, 'step': 1, 'sampling': None}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n",
      "units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 Complete [00h 10m 01s]\n",
      "val_accuracy: 0.001896789064630866\n",
      "\n",
      "Best val_accuracy So Far: 0.001896789064630866\n",
      "Total elapsed time: 00h 39m 20s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train,\n",
    "             epochs=5,\n",
    "            validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [00h 50m 15s]\n",
      "val_accuracy: 0.001896789064630866\n",
      "\n",
      "Best val_accuracy So Far: 0.001896789064630866\n",
      "Total elapsed time: 11h 36m 31s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#with Scaled Data\n",
    "\n",
    "tuner.search(X_trainScaler, y_train,\n",
    "             epochs=10,\n",
    "            validation_data=(X_testScaler, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-4._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-6._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-6.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-6.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-8._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-8.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-8.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-10._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-10.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-10.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-12._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-12.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-12.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-6.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-6.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-7.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-7.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-8.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-8.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-9.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-9.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-10.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-10.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-11.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-11.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-12.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-12.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-6.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-6.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-7.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-7.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-8.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-8.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-9.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-9.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-10.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-10.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-11.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-11.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-12.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-12.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 15,\n",
       " 'units_0': 128,\n",
       " 'units_1': 320,\n",
       " 'learning_rate': 0.01,\n",
       " 'units_2': 32,\n",
       " 'units_3': 32,\n",
       " 'units_4': 32,\n",
       " 'units_5': 32,\n",
       " 'units_6': 32,\n",
       " 'units_7': 32,\n",
       " 'units_8': 32,\n",
       " 'units_9': 32,\n",
       " 'units_10': 32,\n",
       " 'units_11': 32,\n",
       " 'units_12': 32,\n",
       " 'units_13': 32,\n",
       " 'units_14': 32}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameters.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 32.5889\n",
      "Epoch 2/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4060\n",
      "Epoch 3/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4057\n",
      "Epoch 4/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4091\n",
      "Epoch 5/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4050\n",
      "Epoch 6/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4090\n",
      "Epoch 7/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4078\n",
      "Epoch 8/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4007\n",
      "Epoch 9/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4077\n",
      "Epoch 10/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4065\n",
      "Epoch 11/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4036\n",
      "Epoch 12/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4052\n",
      "Epoch 13/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4052\n",
      "Epoch 14/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4079\n",
      "Epoch 15/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4087\n",
      "Epoch 16/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4065\n",
      "Epoch 17/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4062\n",
      "Epoch 18/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4091\n",
      "Epoch 19/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4096\n",
      "Epoch 20/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4079\n",
      "Epoch 21/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4069\n",
      "Epoch 22/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4064\n",
      "Epoch 23/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4086\n",
      "Epoch 24/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4076\n",
      "Epoch 25/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4076\n",
      "Epoch 26/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4079\n",
      "Epoch 27/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4070\n",
      "Epoch 28/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4061\n",
      "Epoch 29/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4069\n",
      "Epoch 30/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4086\n",
      "Epoch 31/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4043\n",
      "Epoch 32/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4072\n",
      "Epoch 33/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4053\n",
      "Epoch 34/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4055\n",
      "Epoch 35/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4088\n",
      "Epoch 36/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4078\n",
      "Epoch 37/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4084\n",
      "Epoch 38/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4093\n",
      "Epoch 39/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4067\n",
      "Epoch 40/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4053\n",
      "Epoch 41/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4101\n",
      "Epoch 42/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4061\n",
      "Epoch 43/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4105\n",
      "Epoch 44/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4072\n",
      "Epoch 45/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4061\n",
      "Epoch 46/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4055\n",
      "Epoch 47/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4055\n",
      "Epoch 48/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4077\n",
      "Epoch 49/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4071\n",
      "Epoch 50/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4072\n",
      "Epoch 51/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4070\n",
      "Epoch 52/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4074\n",
      "Epoch 53/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4083\n",
      "Epoch 54/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4053\n",
      "Epoch 55/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4059\n",
      "Epoch 56/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4087\n",
      "Epoch 57/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4071\n",
      "Epoch 58/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4099\n",
      "Epoch 59/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4077\n",
      "Epoch 60/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4074\n",
      "Epoch 61/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4065\n",
      "Epoch 62/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4071\n",
      "Epoch 63/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4080\n",
      "Epoch 64/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4087\n",
      "Epoch 65/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4062\n",
      "Epoch 66/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4077\n",
      "Epoch 67/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4075\n",
      "Epoch 68/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4064\n",
      "Epoch 69/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4080\n",
      "Epoch 70/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4073\n",
      "Epoch 71/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4083\n",
      "Epoch 72/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4063\n",
      "Epoch 73/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4082\n",
      "Epoch 74/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4075\n",
      "Epoch 75/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4054\n",
      "Epoch 76/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4065\n",
      "Epoch 77/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4053\n",
      "Epoch 78/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4085\n",
      "Epoch 79/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4074\n",
      "Epoch 80/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4052\n",
      "Epoch 81/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4082\n",
      "Epoch 82/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4057\n",
      "Epoch 83/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4076\n",
      "Epoch 84/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4062\n",
      "Epoch 85/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4100\n",
      "Epoch 86/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4076\n",
      "Epoch 87/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4079\n",
      "Epoch 88/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4072\n",
      "Epoch 89/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4055\n",
      "Epoch 90/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4085\n",
      "Epoch 91/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4077\n",
      "Epoch 92/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4062\n",
      "Epoch 93/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4090\n",
      "Epoch 94/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4069\n",
      "Epoch 95/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4067\n",
      "Epoch 96/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4065\n",
      "Epoch 97/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4086\n",
      "Epoch 98/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4080\n",
      "Epoch 99/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4069\n",
      "Epoch 100/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4079\n",
      "Epoch 101/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4057\n",
      "Epoch 102/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4075\n",
      "Epoch 103/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4066\n",
      "Epoch 104/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4075\n",
      "Epoch 105/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4066\n",
      "Epoch 106/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4091\n",
      "Epoch 107/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4086\n",
      "Epoch 108/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4083\n",
      "Epoch 109/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4077\n",
      "Epoch 110/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4083\n",
      "Epoch 111/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4070\n",
      "Epoch 112/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4078\n",
      "Epoch 113/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4063\n",
      "Epoch 114/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4055\n",
      "Epoch 115/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4068\n",
      "Epoch 116/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4066\n",
      "Epoch 117/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4098\n",
      "Epoch 118/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4063\n",
      "Epoch 119/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4090\n",
      "Epoch 120/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4073\n",
      "Epoch 121/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4071\n",
      "Epoch 122/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4079\n",
      "Epoch 123/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4044\n",
      "Epoch 124/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4071\n",
      "Epoch 125/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4088\n",
      "Epoch 126/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4042\n",
      "Epoch 127/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4088\n",
      "Epoch 128/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4082\n",
      "Epoch 129/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4073\n",
      "Epoch 130/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4078\n",
      "Epoch 131/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4075\n",
      "Epoch 132/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4088\n",
      "Epoch 133/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4086\n",
      "Epoch 134/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4065\n",
      "Epoch 135/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4066\n",
      "Epoch 136/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4053\n",
      "Epoch 137/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4068\n",
      "Epoch 138/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4051\n",
      "Epoch 139/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4041\n",
      "Epoch 140/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4064\n",
      "Epoch 141/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4063\n",
      "Epoch 142/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4075\n",
      "Epoch 143/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4089\n",
      "Epoch 144/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4086\n",
      "Epoch 145/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4097\n",
      "Epoch 146/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4081\n",
      "Epoch 147/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4087\n",
      "Epoch 148/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4065\n",
      "Epoch 149/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4088\n",
      "Epoch 150/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4068\n",
      "Epoch 151/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4086\n",
      "Epoch 152/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4081\n",
      "Epoch 153/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4064\n",
      "Epoch 154/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4081\n",
      "Epoch 155/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4075\n",
      "Epoch 156/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4081\n",
      "Epoch 157/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4069\n",
      "Epoch 158/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4072\n",
      "Epoch 159/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4059\n",
      "Epoch 160/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4062\n",
      "Epoch 161/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4067\n",
      "Epoch 162/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4075\n",
      "Epoch 163/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4069\n",
      "Epoch 164/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4077\n",
      "Epoch 165/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4065\n",
      "Epoch 166/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4078\n",
      "Epoch 167/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4069\n",
      "Epoch 168/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4066\n",
      "Epoch 169/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4084\n",
      "Epoch 170/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4077\n",
      "Epoch 171/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4069\n",
      "Epoch 172/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4070\n",
      "Epoch 173/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4088\n",
      "Epoch 174/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4088\n",
      "Epoch 175/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4078\n",
      "Epoch 176/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4076\n",
      "Epoch 177/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4067\n",
      "Epoch 178/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4086\n",
      "Epoch 179/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4063\n",
      "Epoch 180/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4075\n",
      "Epoch 181/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4067\n",
      "Epoch 182/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4054\n",
      "Epoch 183/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4066\n",
      "Epoch 184/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4079\n",
      "Epoch 185/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4080\n",
      "Epoch 186/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4073\n",
      "Epoch 187/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4080\n",
      "Epoch 188/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4073\n",
      "Epoch 189/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4052\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4073\n",
      "Epoch 191/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4052\n",
      "Epoch 192/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4066\n",
      "Epoch 193/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4026\n",
      "Epoch 194/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4068\n",
      "Epoch 195/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4075\n",
      "Epoch 196/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4080\n",
      "Epoch 197/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4066\n",
      "Epoch 198/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4088\n",
      "Epoch 199/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4064\n",
      "Epoch 200/200\n",
      "9111/9111 [==============================] - 22s 2ms/step - loss: 1.4081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f595c8a8470>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "modeltuned = Sequential()\n",
    "# define first hidden layer and visible layer\n",
    "modeltuned.add(Dense(128, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define second hidden layer and visible layer\n",
    "modeltuned.add(Dense(320, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define third hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define fourth hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define fifth hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define sixth hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define seventh hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define eighth hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define ninth hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define tenth hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define eleventh hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define twelveth hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define thirteen hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define fourteen hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define fifteen hidden layer and visible layer\n",
    "modeltuned.add(Dense(32, input_dim=n_inputs, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define output layer\n",
    "modeltuned.add(Dense(1, activation='sigmoid'))\n",
    "# define loss and optimizer\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "modeltuned.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "modeltuned.fit(X_trainScaler,y_train,class_weight=weights_assigned, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred2=modeltuned.predict(X_testScaler)\n",
    "print(roc_auc_score(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0 124711]\n",
      " [     0    237]]\n",
      "237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00    124711\n",
      "           1       0.00      1.00      0.00       237\n",
      "\n",
      "    accuracy                           0.00    124948\n",
      "   macro avg       0.00      0.50      0.00    124948\n",
      "weighted avg       0.00      0.00      0.00    124948\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvisi/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, accuracy_score,classification_report\n",
    "print(confusion_matrix(y_test, y_pred2.round(), normalize=None))\n",
    "print(accuracy_score(y_test, y_pred2.round(), normalize=False))\n",
    "print(classification_report(y_test, y_pred2.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 1, 1: 0.2})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight=dict({0:1,1:0.2}) # Hardcoding and giving 100 times more weight to the less represtend 1 class \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier(class_weight=class_weight)\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124709      2]\n",
      " [   229      8]]\n",
      "0.9981512309120594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    124711\n",
      "           1       0.80      0.03      0.06       237\n",
      "\n",
      "    accuracy                           1.00    124948\n",
      "   macro avg       0.90      0.52      0.53    124948\n",
      "weighted avg       1.00      1.00      1.00    124948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predRF=classifier.predict(X_test) \n",
    "print(confusion_matrix(y_test,predRF)) \n",
    "print(accuracy_score(y_test,predRF)) \n",
    "print(classification_report(y_test,predRF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 290937, 1: 606})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvisi/.local/lib/python3.6/site-packages/imblearn/utils/_validation.py:591: FutureWarning: Pass sampling_strategy=1 as keyword args. From version 0.9 passing these as positional arguments will result in an error\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes before fit Counter({0: 290937, 1: 606})\n",
      "The number of classes after fit Counter({0: 290937, 1: 290937})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "os=RandomOverSampler(1)\n",
    "X_train_os,y_train_os=os.fit_resample(X_train,y_train)\n",
    "print(\"The number of classes before fit {}\".format(Counter(y_train)))\n",
    "print(\"The number of classes after fit {}\".format(Counter(y_train_os)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifieros=RandomForestClassifier()\n",
    "classifieros.fit(X_train_os,y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124702      9]\n",
      " [   219     18]]\n",
      "0.9981752409002145\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    124711\n",
      "           1       0.67      0.08      0.14       237\n",
      "\n",
      "    accuracy                           1.00    124948\n",
      "   macro avg       0.83      0.54      0.57    124948\n",
      "weighted avg       1.00      1.00      1.00    124948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predRF_os=classifieros.predict(X_test) \n",
    "print(confusion_matrix(y_test,predRF_os)) \n",
    "print(accuracy_score(y_test,predRF_os)) \n",
    "print(classification_report(y_test,predRF_os))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "plot_roc_curve(classifieros, X_test, y_test) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=16, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying XGBoost Classification\n",
    "import xgboost as xgb\n",
    "XGBClassifier = xgb.XGBClassifier()\n",
    "XGBClassifier.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[122879   1832]\n",
      " [   127    110]]\n",
      "0.9843214777347377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    124711\n",
      "           1       0.06      0.46      0.10       237\n",
      "\n",
      "    accuracy                           0.98    124948\n",
      "   macro avg       0.53      0.72      0.55    124948\n",
      "weighted avg       1.00      0.98      0.99    124948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predXGB_os=XGBClassifier.predict(X_test) \n",
    "print(confusion_matrix(y_test,predXGB_os)) \n",
    "print(accuracy_score(y_test,predXGB_os)) \n",
    "print(classification_report(y_test,predXGB_os))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "plot_roc_curve(XGBClassifier, X_test, y_test) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes before fit Counter({0: 290937, 1: 606})\n",
      "The number of classes after fit Counter({0: 290910, 1: 290910})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "sos=SMOTETomek(1)\n",
    "X_train_sos,y_train_sos=sos.fit_resample(X_train,y_train)\n",
    "print(\"The number of classes before fit {}\".format(Counter(y_train)))\n",
    "print(\"The number of classes after fit {}\".format(Counter(y_train_sos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:49:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=16, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying XGBoost Classification\n",
    "import xgboost as xgb\n",
    "XGBClassifier_sos = xgb.XGBClassifier()\n",
    "XGBClassifier_sos.fit(X_train_sos, y_train_sos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124679     32]\n",
      " [   222     15]]\n",
      "0.9979671543362039\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    124711\n",
      "           1       0.32      0.06      0.11       237\n",
      "\n",
      "    accuracy                           1.00    124948\n",
      "   macro avg       0.66      0.53      0.55    124948\n",
      "weighted avg       1.00      1.00      1.00    124948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predXGB_sos=XGBClassifier_sos.predict(X_test) \n",
    "print(confusion_matrix(y_test,predXGB_sos)) \n",
    "print(accuracy_score(y_test,predXGB_sos)) \n",
    "print(classification_report(y_test,predXGB_sos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So SMOTETomek is not giving good result in terms of Recall with XGBoost although gave good accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuSklEQVR4nO3deXwU5f3A8c83ByQEwpWACsRw33LFk1pBxIIHVKUiHi0tLRa1WrVa+vOqeLQoKrWeeEBtFfEWEcQLRFGUGyEgtxDuIxwhJCTZ7++PmaybEJKFZHaz2e/79doXOzPPzHwnCfvdeZ5nnkdUFWOMMdErJtwBGGOMCS9LBMYYE+UsERhjTJSzRGCMMVHOEoExxkS5uHAHcLxSUlI0PT093GEYY0xEWbhw4W5VTS1rW8QlgvT0dBYsWBDuMIwxJqKIyI/H2mZVQ8YYE+UsERhjTJSzRGCMMVHOEoExxkQ5SwTGGBPlPEsEIvKyiOwUkeXH2C4i8qSIrBWRZSLS06tYjDHGHJuXdwSTgAHlbB8ItHVfI4FnPYzFGGPMMXj2HIGqzhGR9HKKDAZeUWcc7Hki0kBETlbVbV7FZIwxVWndrhzeX7IVQjScf7+OTenWokGVHzecD5Q1AzYHLGe5645KBCIyEueugbS0tJAEZ4yp/p74ZDWrdxwM2/lnLN8OgEhoztckOaHGJYKgqeoEYAJARkaGzaRjTDWnqsxYvp3HP1mNl5Nfrdt1CIB2Tet6do7ytG1Sl5+1TeG+SzuH5fxVJZyJYAvQImC5ubvOGBMBdh3MZ++hI2Vuu+LZr8nJLwTg4tNO9iyGTqfU5/qft6JLs/qenSMahDMRTAVuEpHXgTOB/dY+YEz14fMpX6zZxSH3Az1QQZGPW6csLXf/eglxPHV1T85rV+Y4Z6Ya8SwRiMhkoA+QIiJZwH1APICqPgdMBy4C1gK5wG+9isUYE5xpy7ayekcOAIs3ZfPlmt3llu/TPpUrM1octT5GhN5tGlMvId6TOE3V8rLX0LAKtitwo1fnN8aUb+GP2bz81QaUn+rwp3+//ahyL/46g1Mb1zlqfWyM0DIlCQlVS6nxTEQ0Fhtjqt6fXlvE1v15tG3yU0Nr+6b1uOvijvzcqnOiiiUCY2qoj5Zv58bXFlHkO3avnaRasXxy23khjMpUR5YIjIlwX6/dTXZuwVHrP1qxnSKfcvP5bY5ZfXNh56Zeh2cigCUCYyLMJ5k7WLF1PwCb9x7m7UVZxyzboE48t/ZvZ/X4plyWCIyJAPtzC3jgw0wOHyniw++P7mX9yJDT6F7GE6cpdWtbEjAVskRgTDXz17eWsWhTdolhC4q7dNaKjaFd07r87aKO9Alo0LUPe1MZlgiMCbGCIh8/7jlU5rY/TV7Cym0HABjY5ST/+tapdWlQpxb3D+pMrTibRsRULUsExoTQtv2HGfLsN2zZd7jccrP/0of0lKQQRWWinSUCYzy2flcO05ZtQxWe+HS1f/2/h/Uos/xZrRqTWq92qMIzxhKBMVUtJ7+QBz7IJOeIM0bPh8tKNu72bZ/Kk8N62PALptqwRGBMJWRuPcCdby/lSKEPwWmw/cEdH18EWqUk0SoliTNaNuLhy7oCEBNjDbumerFEYMwJ8vmUi578EoAeaQ1oWi8BgJYpSdRPjOf+wZ1JiI8NZ4jGBMUSgTEV2Hkwj4Ubs49an1/oA5zJSd4ZdY514TQRyxKBMeWYu3Y317z4bbllbipnCAdjIoElAmOOIfdIoT8JdD4lmceu7HZUmbiYGFqnWjdPE9ksERhTyk2vLWLNjhx/o+8VPZsz7len2bd+U2NZIjAmwMcrtjPN7e75i85NqVMrjrsv7mhJwNRolghM1Ms+dIRvN+xl+/7D/P2DTADe/OPZnJ7eKMyRGRMalghMVFm+ZT+zVu0sse6xT1aXWB7Vp7UlARNVLBGYGmdZ1j5e+moDZU3M9cHSrWXuc0r9BF4afjoJ8bG0tDF+TJSxRGBqnNvfWMqanTm0KuMDvWVKEpd2O4Wbz29TYn1sjFg7gIlalghMxMs+dITs3CNk5xZw5fPfUORTYmOEz//SJ9yhGRMRLBGYiKWqzF69i99OnF9ifYeT6vHw5V3DFJUxkccSgYlYz8xex6MzfwDg3LYpDOnVnNpxsZzfoYlN3mLMcbBEYCLKi1+u5/stzsTt7y9xGn6furoHF3c92er4jTlBlghMRPD5lFGvLmTmih0ApDeuQ3rjOtzxiw5cfNrJYY7OmMhmicCExa6D+eTkF/qXdxzI45oXv6XIp8THHv3NvqDI6QsaI/DG9WeTYf38jakylghMyGVuPeAfx7+07i0acE7rxmVui4+N4TfnpNMoqZaX4RkTdSwRmJAb9epCAK7MaM45rVP86xNrxdKvQxPiYq2h15hQskRgQsbnU/4xYyU/7skFYOwVNqKnMdWBJQITEp+v2sHot79n58F8AMZe0dWSgDHVhKf34CIyQER+EJG1IjK6jO1pIjJLRBaLyDIRucjLeEz4/G7SAnYezOeiricx546+DD09LdwhGWNcnt0RiEgs8DTQH8gC5ovIVFXNDCh2N/CGqj4rIp2A6UC6VzGZ0CryKfPW7/H3+2/XtC7PXNMrzFEZY0rzsmroDGCtqq4HEJHXgcFAYCJQINl9Xx8oe2hIEzHmb9zLdxv2AjBv/R6+XLPbv+3eSzqHKyxjTDm8TATNgM0By1nAmaXK/B34WET+BCQBF5R1IBEZCYwESEuzKoXq6tnZ6xj70aqj1k+4rhdtmtSlVWrdMERljKlIuBuLhwGTVPUxETkb+K+IdFFVX2AhVZ0ATADIyMgoY5R5Ew65RwoZ+cpC9h46ggis2HoAgL8O6MDvfpYOQKyIdQc1pprzMhFsAVoELDd31wUaAQwAUNVvRCQBSAF2YqqlI4U+tu0/DMA976/gq7W7iYsR+rRP5ZQGidzYtw3dWzQIb5DGmOPiZSKYD7QVkZY4CeAq4OpSZTYB/YBJItIRSAB2eRiTqYTt+/O46Mkv2XvoSIn1i+7tT3JCfJiiMsZUlmeJQFULReQmYCYQC7ysqitEZAywQFWnArcDL4jIrTgNx8NV1ap+qqHCIh9n/eMzAETgsV91A6BHWkNLAsZEOE/bCFR1Ok6X0MB19wa8zwR6exmDOTGFRT7+MWMV2e63/2VuF9DE+Fjm330BdWuHu3nJGFNV7H+zKWHi3A28tTCLrOzD7D9cAECLRon4fNCmSV0mDj/dkoAxNYz9jzZ+U5du5f4PnMc8+nVoQu34GO66uBPNGiSGOTJjjJcsERi/mSu2A/DaH84sMSqoMaZmsw7eBoCs7Fw+XLYNwJKAMVHGEoEB4I43lwFw7Vn25LYx0caqhqLQnpx8bnptMYeO/DRV5LIsp1fQg7/sGq6wjDFhYomgBssvLCIr+zBXPPs1B/MK/XMB5xU4I3g0Ta5Np5OdMf/6tk9lYFebBN6YaGSJoAbavDeXNTsPcvsbS8nOdbqAptStxRU9m/vL1EuI4/rzWhNv4wAZE/UsEdQwB/IKOPeRWf7lk5ITGD2wA/07NSXJ+v8bY8pgnww1RH5hEf+csYo35jsjf2ec2pB7LulE+5PqkRAfG+bojDHVWdCJQETqqGqul8GYEzP+09VMmLOe3CNFAJye3pApI88mJsbmBDbGVKzCRCAi5wAvAnWBNBHpBlyvqjd4HZyp2B1vLuXNhVkADOxyEvcP6kyT5IQwR2WMiSTB3BE8AfwCmAqgqktF5OeeRmWCMm/9Hn8SeO/G3jYPgDHmhATVZURVN5daVeRBLOY4TFu2lasmzAPgros6WhIwxpywYO4INrvVQyoi8cAtwEpvwzLlOZBXwE2vLQbgd71b8vtzW4Y5ImNMJAsmEfwR+BfOZPRbgI8Bax8Io4UbswF4YHBnrjs7PbzBGGMiXjCJoL2qXhO4QkR6A3O9CclU5J73lwPQvUXDMEdijKkJgmkj+HeQ64zHdh7IY9LcDWRlO5PHd21eP8wRGWNqgmPeEYjI2cA5QKqI3BawKRlnDmITAmt3HmTWql0APDT9p6aZce6cwcYYU1nlVQ3Vwnl2IA6oF7D+ADDEy6AMFPmUR2au4vkv1pdY3/mUZB6/sjvtmtYNU2TGmJrmmIlAVb8AvhCRSar6YwhjMsDHK7b7k8CvejXnvkGdAUiqFYuIPTFsjKk6wTQW54rIo0BnwP/Iqqqe71lUUSwnv5ALHvuC7QfyAJhxy7l0dIeKNsYYLwTTWPwqsApoCdwPbATmexhTVJu1aifbD+RRPzGe+y7tZEnAGOO5YO4IGqvqSyJyS0B1kSWCKrZ2Zw6zf9jJgx86DcL/G3Gm9QoyxoREMImgwP13m4hcDGwFGnkXUnT63aT5bNrrDO7a4aR6lgSMMSETTCJ4UETqA7fjPD+QDPzZy6CixaY9ufz17WUcLihi095c4mOFRff0J6mWTRNhjAmdCj9xVHWa+3Y/0Bf8TxabE7RuVw7DJsxj58F8AJo1SOTctilcc+ap1EuID3N0xphoU94DZbHAlThjDH2kqstF5BLg/4BEoEdoQqx5Fm/ax86D+WSc2pDz2qVyQ982xNokMsaYMCnvjuAloAXwHfCkiGwFMoDRqvpeCGKrcbbsO8xHy7fzwLRMAB4ZchqtUu3BMGNMeJWXCDKA01TVJyIJwHagtaruCU1oNYeq8uRna3ni09X+dZ1OTrYkYIypFspLBEdU1Qegqnkisv54k4CIDMAZwjoWeFFV/1lGmSuBvwMKLFXVq4/nHNXZ/737Pau2HWDf4QLW7zoEwHntUvn31T2oaw3CxphqorxPow4issx9L0Brd1kAVdXTyjuw28bwNNAfyALmi8hUVc0MKNMW+BvQW1WzRaRJJa6l2nnt2000b5hIy5QkTm1Uh/+7qCNtm9areEdjjAmh8hJBx0oe+wxgraquBxCR14HBQGZAmT8AT6tqNoCq7qzkOauFI4U+nv9iHQAXdz2Zv11U2R+lMcZ4p7xB5yo70FwzIHCu4yzgzFJl2gGIyFyc6qO/q+pHpQ8kIiOBkQBpaWmVDMt77y3ZwmOfOO0Bl3Y7JczRGGNM+YKavN5DcUBboA8wDHhBRBqULqSqE1Q1Q1UzUlNTQxvhCfhyzW4Apv3pZ3RpZk8IG2OqNy8TwRac7qfFmrvrAmUBU1W1QFU3AKtxEkPE8vmUD5ZuBZyeQcYYU90FlQhEJFFE2h/nsecDbUWkpYjUAq4CppYq8x7O3QAikoJTVbSeCPbuYifX9UhrQIw9JGaMiQAVJgIRuRRYAnzkLncXkdIf6EdR1ULgJmAmsBJ4Q1VXiMgYERnkFpsJ7BGRTGAWcEekP6cwY/l2AJ6+umeYIzHGmOAE05n97zg9gGYDqOoSEWkZzMFVdTowvdS6ewPeK3Cb+4poa3fm8PmqHXy6cgcxAqc0SAx3SMYYE5SghqFW1f2lpkdUj+KJSO8syuK2N5b6l/86oEMYozHGmOMTTCJYISJXA7HuA2A3A197G1bk+Mf0lTw/x2nWuLxHM+4f3NlGEDXGRJRgGov/hDNfcT7wGs5w1H/2MKaI8so3zuMWL/46g8eHdrckYIyJOMHcEXRQ1buAu7wOJtIcKfRxuKCIq05vwQWdmoY7HGOMOSHB3BE8JiIrReQBEenieUQR5NsNTgcnm0vAGBPJKkwEqtoXZ2ayXcDzIvK9iNzteWTV3LpdOVz30ncAXJnRooLSxhhTfQX1QJmqblfVJ4E/4jxTcG/5e9R8Q5512st7pjXgNJto3hgTwYJ5oKyjiPxdRL7Hmbz+a5zhIqLavsMF1KkVyzs39KZU11pjjIkowTQWvwxMAX6hqls9jqday8rOZdOeXHwKqvDH81qHOyRjjKm0ChOBqp4dikCqO1XlZ2NnlVjXuG6tMEVjjDFV55iJQETeUNUr3SqhwCeJg5qhrKZZsfUAAE2Ta/PkVT2IixVOa94gvEEZY0wVKO+O4Bb330tCEUh1t2G3M+fwmMFdOLNV4zBHY4wxVae8Gcq2uW9vUNW/Bm4TkbHAX4/eq2a6/Jm5/juC9jbnsDGmhgmm+2j/MtYNrOpAqqvvs/azaNM+8gt93HpBO9Ia1Ql3SMYYU6XKayMYBdwAtBKRZQGb6gFzvQ6sunjskx8A+O+IMzi3bfWfJtMYY45XeW0ErwEzgH8AowPWH1TVvZ5GVU08/8U6Zv+wC4CftUkJczTGGOON8hKBqupGEbmx9AYRaVTTk8GWfYf5x4xVAPz67FPtoTFjTI1V0R3BJcBCnO6jgZ+ECrTyMK6we2tBFgAP/LIL1511apijMcYY75TXa+gS99+gpqWsSbbtP8y6XTkAXNGzWZijMcYYb1X4ZLGI9AaWqOohEbkW6AmMV9VNnkcXYoePFPHB0q3c+bbTNt6sQSJ1agUzCocxxkSuYLqPPgvkikg34HZgHfBfT6MKk5e+Wu9PAme1asSU688Kc0TGGOO9YL7uFqqqishg4ClVfUlERngdWKh9t2Ev4z5eDcCcO/rSolGiNRAbY6JCMIngoIj8DbgOOFdEYoAaNzHvvz5zksDverckrbE9NGaMiR7BVA0NxZm4/nequh1nLoJHPY0qxFSVnQfyAbj30k5hjsYYY0IrmKkqtwOvAvVF5BIgT1Vf8TyyEMkrKOKfH61izc4cmibXDnc4xhgTcsHMUHYl8B3wK+BK4FsRGeJ1YKGgqlzz4rc8/8V6AP5xedcwR2SMMaEXTBvBXcDpqroTQERSgU+Bt7wMLBQ27c1l4Y/ZgNNAbG0DxphoFEwbQUxxEnDtCXK/am/J5n0ATLiulyUBY0zUCuaO4CMRmQlMdpeHAtO9Cyk0VJVbXl8CYEnAGBPVgpmz+A4RuRz4mbtqgqq+621Y3iueaKZZg0SbbMYYE9XKm4+gLTAOaA18D/xFVbeEKjCvvb/EuZT7B3W2B8eMMVGtvLr+l4FpwBU4I5D++3gPLiIDROQHEVkrIqPLKXeFiKiIZBzvOU5U8Yf/z9raPAPGmOhWXtVQPVV9wX3/g4gsOp4Di0gs8DTOVJdZwHwRmaqqmaXK1QNuAb49nuNX1uerdlKvdhwJ8bGhPK0xxlQ75SWCBBHpwU/zECQGLqtqRYnhDGCtqq4HEJHXgcFAZqlyDwBjgTuOM/ZKSYiPIT6uRnR+MsaYSikvEWwDHg9Y3h6wrMD5FRy7GbA5YDkLODOwgIj0BFqo6ocicsxEICIjgZEAaWlpFZy2Yj6fsnzLAQZ2OanSxzLGmEhX3sQ0fb08sTt43ePA8IrKquoEYAJARkaGVvbcc9ftBqCgqNKHMsaYiOdl3cgWoEXAcnN3XbF6QBdgtohsBM4CpoaiwfjHPbkA/PG8Gj3bpjHGBMXLRDAfaCsiLUWkFnAVMLV4o6ruV9UUVU1X1XRgHjBIVRd4GBMA+YU+ANo2secHjDHGs0SgqoXATcBMYCXwhqquEJExIjLIq/MGY82OgwAk1rIeQ8YYE8ycxQJcA7RS1TEikgacpKrfVbSvqk6n1HAUqnrvMcr2CSriKpCdewSAWtZryBhjgrojeAY4GxjmLh/EeT4gYh04XBjuEIwxptoIZtC5M1W1p4gsBlDVbLfOP2J9t3EvnU5ODncYxhhTLQRzR1DgPiWs4J+PwOdpVB5SVYp8Sl5hUbhDMcaYaiGYRPAk8C7QREQeAr4CHvY0Kg8dKXJy2BnpjcIciTHGVA/BDEP9qogsBPrhDC/xS1Vd6XlkHjnidh1tnVo3zJEYY0z1EEyvoTQgF/ggcJ2qbvIyMK9s2H0I+OnOwBhjol0wjcUf4rQPCJAAtAR+ADp7GJdntu3PA6BLs/phjsQYY6qHYKqGugYuuwPF3eBZRB4rrhqqYw+TGWMMcAJPFrvDT59ZYcFqKq/A6S3UpF7tMEdijDHVQzBtBLcFLMYAPYGtnkXksc17nQHnasfZHYExxkBwbQSBI7MV4rQZvO1NON7bfcgZXqJhUnyYIzHGmOqh3ETgPkhWT1X/EqJ4QqZWrI0zZIwxUE4bgYjEqWoR0DuE8Xhu7Y4cUurW9k9eb4wx0a68O4LvcNoDlojIVOBN4FDxRlV9x+PYPNGgTjyH8m3QOWOMKRZMG0ECsAdnjuLi5wkUiMhEUFDko21Te6rYGGOKlZcImrg9hpbzUwIoFrGT/RYUKXExVi1kjDHFyksEsUBdSiaAYhGbCFZtP0jr1KRwh2GMMdVGeYlgm6qOCVkkIZKcGMf+wwXhDsMYY6qN8vpQ1sj6k72HjtD5FBtnyBhjipWXCPqFLIoQ2pdbgGrE1mwZY0yVO2YiUNW9oQwkFNbvygGgaf2EMEdijDHVR1Q9XvvB0m0AnNHSZiczxphiUZUICn0+RKBv+ybhDsUYY6qNqEoEBw4X1MwWcGOMqYSoSgTfb9mPz9qJjTGmhKhKBA3r1LIJaYwxppSoSgQ+VU6yHkPGGFNCVCWCNTtzwh2CMcZUO1GVCFLr1SY790i4wzDGmGolqhKBz6e0TrUhqI0xJpCniUBEBojIDyKyVkRGl7H9NhHJFJFlIvKZiJzqZTyFPhuC2hhjSvMsEbjzHT8NDAQ6AcNEpFOpYouBDFU9DXgLeMSreACKfEqsJQJjjCnByzuCM4C1qrpeVY8ArwODAwuo6ixVzXUX5wHNPYyHQksExhhzFC8TQTNgc8BylrvuWEYAM8raICIjRWSBiCzYtWvXCQd0MK+A2JioahYxxpgKVYtPRRG5FsgAHi1ru6pOUNUMVc1ITU094fPsOJBPrk1cb4wxJQQzef2J2gK0CFhu7q4rQUQuAO4CzlPVfA/jISE+hoZJtbw8hTHGRBwv7wjmA21FpKWI1AKuAqYGFhCRHsDzwCBV3elhLADEiNCwTrzXpzHGmIjiWSJQ1ULgJmAmsBJ4Q1VXiMgYERnkFnsUqAu8KSJLRGTqMQ5XJXyqxIg1FhtjTCAvq4ZQ1enA9FLr7g14f4GX5y/NpyCWCIwxpoRq0VgcKqqK9R41xpiSoioR+BSrGjLGmFKiKhEU+RTLA8YYU1LUJAJVZ2qy3Tk2+qgxxgSKmkRQ6M5ReYpNTGOMMSVETSIochNBXGzUXLIxxgQlaj4Vi+8IbBhqY4wpKWoSQVGRkwhs9FFjjCkpahJBoc8HQFysJQJjjAkUNYlgy77DgDNdpTHGmJ9ETSLIcYefPql+YpgjMcaY6iVqEoFbM0QjG4baGGNKiJpEUKTFjcVhDsQYY6qZqPlYLG4bsLGGjDGmpKhJBMUPlFn3UWOMKSl6EoHaHYExxpQlahKBz+4IjDGmTFGTCPYcckYdtURgjDElRU0iqFvbmZVT7XkyY4wpIWoSgeJkgNpxUXPJxhgTlKj5VCx+oMwai40xpqS4cAcQKj63TsjygDkRBQUFZGVlkZeXF+5QjClXQkICzZs3Jz4+Puh9oiYRFDcNWCIwJyIrK4t69eqRnp6O2B+RqaZUlT179pCVlUXLli2D3i9qqobUniMwlZCXl0fjxo0tCZhqTURo3Ljxcd+5Rk0iKB592hKBOVGWBEwkOJG/0yhKBNZGYIwxZYmaRFD8/IAlAhOJNm/eTMuWLdm7dy8A2dnZtGzZko0bNwKwZs0aLrnkElq3bk2vXr3o27cvc+bMAWDSpEmkpqbSvXt3OnfuzJAhQ8jNzfUfe9y4cXTo0IHu3btz+umn88orrwDQp08fFixYUCXxL1iwgJtvvhmA/Px8LrjgArp3786UKVP4/e9/T2ZmZqWOP378eH/cAIWFhaSmpjJ69OgS5dLT09m9e7d/efbs2VxyySX+5RkzZpCRkUGnTp3o0aMHt99+e6XiAli4cCFdu3alTZs23Hzzzf5q6kD79+/n0ksvpVu3bnTu3JmJEyf6t8XGxtK9e3e6d+/OoEGD/Ouvuuoq1qxZU+n4AKfuPJJevXr10hPxytcb9NS/TtNdB/NOaH8T3TIzM8Mdgo4dO1b/8Ic/qKrqyJEj9eGHH1ZV1cOHD2vbtm31/fff95f9/vvvdeLEiaqqOnHiRL3xxhv924YNG6Yvv/yyqqo+++yzeuGFF+r+/ftVVXX//v06adIkVVU977zzdP78+VV+Hd98843269fvhPcvLCwssVxQUKBdu3bVgoIC/7rp06frOeeco61atVKfz+dff+qpp+quXbv8y7NmzdKLL75YVZ2fWatWrXTlypX+8zzzzDMnHGex008/Xb/55hv1+Xw6YMAAnT59+lFlHnroIb3zzjtVVXXnzp3asGFDzc/PV1XVpKSkMo87e/Zs/f3vf1/mtrL+XoEFeozP1ajpNWRtBKaq3P/BCjK3HqjSY3Y6JZn7Lu1cbplbb72VXr16MX78eL766iueeuopAF599VXOPvvsEt8Wu3TpQpcuXY46RmFhIYcOHaJhw4YAPPzww8yePZvk5GQAkpOT+c1vfnPUfqNGjWL+/PkcPnyYIUOGcP/99wMwevRopk6dSlxcHBdeeCHjxo3jzTff5P777yc2Npb69eszZ84cZs+ezbhx43j55Ze59tpr2bVrF927d+ftt99mxIgRjBs3joyMDD7++GPuu+8+8vPzad26NRMnTqRu3bqkp6czdOhQPvnkE+68806uuuoqf2yff/45PXv2JC7up4+zyZMnc8stt/Dss8/yzTffcM4551T4O3jkkUe466676NChA+B8Ex81alSF+5Vn27ZtHDhwgLPOOguAX//617z33nsMHDiwRDkR4eDBg6gqOTk5NGrUqMT1lOXcc89l+PDhFBYWVli2IlGUCNw2gjDHYcyJio+P59FHH2XAgAF8/PHH/n7iK1asoGfPnuXuO2XKFL766iu2bdtGu3btuPTSSzlw4AAHDx6kVatWFZ77oYceolGjRhQVFdGvXz+WLVtGs2bNePfdd1m1ahUiwr59+wAYM2YMM2fOpFmzZv51xZo0acKLL77IuHHjmDZtWoltu3fv5sEHH+TTTz8lKSmJsWPH8vjjj3PvvfcC0LhxYxYtWnRUbHPnzqVXr17+5by8PD799FOef/559u3bx+TJk4NKBMuXLw+qKmjWrFnceuutR62vU6cOX3/9dYl1W7ZsoXnz5v7l5s2bs2XLlqP2vemmmxg0aBCnnHIKBw8eZMqUKcTExPivJyMjg7i4OEaPHs0vf/lLAGJiYmjTpg1Lly4tcf0nImoSgdodgakiFX1z99KMGTM4+eSTWb58Of379y+zzGWXXcaaNWto164d77zzDgBDhw7lqaeeQlW58cYbefTRR7nhhhuCPu8bb7zBhAkTKCwsZNu2bWRmZtKpUycSEhIYMWIEl1xyib+uvXfv3gwfPpwrr7ySyy+/POhzzJs3j8zMTHr37g3AkSNHOPvss/3bhw4dWuZ+27Zto2PHjv7ladOm0bdvXxITE7niiit44IEHGD9+PLGxsWX2qDneXjZ9+/ZlyZIlx7VPRWbOnEn37t35/PPPWbduHf379+fcc88lOTmZH3/8kWbNmrF+/XrOP/98unbtSuvWrQEnsW7durXSicDTxmIRGSAiP4jIWhEZXcb22iIyxd3+rYikexWL/44gaprHTU2zZMkSPvnkE+bNm8cTTzzBtm3bAOjcuXOJb8rvvvsukyZN8jcsBxIRLr30UubMmUNycjJ169Zl/fr15Z53w4YNjBs3js8++4xly5Zx8cUXk5eXR1xcHN999x1Dhgxh2rRpDBgwAIDnnnuOBx98kM2bN9OrVy/27NkT1PWpKv3792fJkiUsWbKEzMxMXnrpJf/2pKSkMvdLTEws0W9+8uTJfPrpp6Snp/vP//nnnwPOXUV2dra/7N69e0lJSQGcn+PChQsrjHPWrFn+xtvAV1l3Hc2aNSMrK8u/nJWVRbNmzY4qN3HiRC6//HJEhDZt2tCyZUtWrVrlPwZAq1at6NOnD4sXL/bvl5eXR2JiYoUxV8Szj0URiQWeBgYCnYBhItKpVLERQLaqtgGeAMZ6FY/dEZhIpqqMGjWK8ePHk5aWxh133MFf/vIXAK6++mrmzp3L1KlT/eUDewWV9tVXX/m/Uf7tb3/jxhtv5MABp80jJyenRO8bgAMHDpCUlET9+vXZsWMHM2bM8Jfdv38/F110EU888QRLly4FYN26dZx55pmMGTOG1NRUNm/eHNQ1nnXWWcydO5e1a9cCcOjQIVavXl3hfh07dvTvc+DAAb788ks2bdrExo0b2bhxI08//TSTJ08GnJ5Q//3vfwEoKirif//7H3379gXgjjvu4OGHH/af0+fz8dxzzx11vuI7gtKv0tVCACeffDLJycnMmzcPVeWVV15h8ODBR5VLS0vjs88+A2DHjh388MMPtGrViuzsbPLz8wGn6mzu3Ll06vTTx+jq1avLbAs6Xl5WDZ0BrFXV9QAi8jowGAjsJzYY+Lv7/i3gKRERt4W7SlkbgYlkL7zwAmlpaf7qoBtuuIGJEyfyxRdfcN555zFt2jRuu+02/vznP9O0aVPq1avH3Xff7d+/uI3A5/PRvHlzJk2aBDiNwDk5OZx++unEx8cTHx9/VD15t27d6NGjBx06dKBFixb+qpuDBw8yePBg8vLyUFUef/xxwPlAXbNmDapKv3796NatG1988UWF15iamsqkSZMYNmyY/8PvwQcfpF27duXuN3DgQK677jrAuRs6//zzqV27tn/74MGDufPOO8nPz+eee+5h1KhRdOvWDVVlwIABXHvttQCcdtppjB8/nmHDhpGbm4uIlOhaeqKeeeYZhg8fzuHDhxk4cKC/obg4yfzxj3/knnvuYfjw4XTt2hVVZezYsaSkpPD1119z/fXXExMTg8/nY/To0f5EsGPHDhITEznppJMqHaN48JnrHFhkCDBAVX/vLl8HnKmqNwWUWe6WyXKX17lldpc61khgJEBaWlqvH3/88bjj+SRzB+8t3sJjV3YjIT72RC/LRKmVK1eWqIc21ctll13GI488Qtu2bcMdSsg88cQTJCcnM2LEiKO2lfX3KiILVTWjrGNFRI25qk5Q1QxVzUhNTT2hY/Tv1JSnr+lpScCYGuif//ynv80kWjRo0KDMrr4nwsuqoS1Ai4Dl5u66sspkiUgcUB8IrmXJGGNc7du3p3379uEOI6R++9vfVtmxvLwjmA+0FZGWIlILuAqYWqrMVKA4pQ0BPveifcCYqmB/miYSnMjfqWeJQFULgZuAmcBK4A1VXSEiY0Sk+BHIl4DGIrIWuA04qoupMdVBQkICe/bssWRgqjV15yNISEg4rv08ayz2SkZGhlbVQFjGBMtmKDOR4lgzlJXXWBw1TxYbUxnx8fHHNeOTMZEkInoNGWOM8Y4lAmOMiXKWCIwxJspFXGOxiOwCjv/RYkcKsLvCUjWLXXN0sGuODpW55lNVtcwnciMuEVSGiCw4Vqt5TWXXHB3smqODV9dsVUPGGBPlLBEYY0yUi7ZEMCHcAYSBXXN0sGuODp5cc1S1ERhjjDlatN0RGGOMKcUSgTHGRLkamQhEZICI/CAia0XkqBFNRaS2iExxt38rIulhCLNKBXHNt4lIpogsE5HPROTUcMRZlSq65oByV4iIikjEdzUM5ppF5Er3d71CRF4LdYxVLYi/7TQRmSUii92/74vCEWdVEZGXRWSnO4NjWdtFRJ50fx7LRKRnpU+qqjXqBcQC64BWQC1gKdCpVJkbgOfc91cBU8IddwiuuS9Qx30/Khqu2S1XD5gDzAMywh13CH7PbYHFQEN3uUm44w7BNU8ARrnvOwEbwx13Ja/550BPYPkxtl8EzMCZgv0s4NvKnrMm3hGcAaxV1fWqegR4HRhcqsxg4D/u+7eAfiISyfPaV3jNqjpLVXPdxXk4M8ZFsmB+zwAPAGOBmjB+dDDX/AfgaVXNBlDVnSGOsaoFc80KJLvv6wNbQxhflVPVOcDecooMBl5RxzyggYicXJlz1sRE0AzYHLCc5a4rs4w6E+jsBxqHJDpvBHPNgUbgfKOIZBVes3vL3EJVPwxlYB4K5vfcDmgnInNFZJ6IDAhZdN4I5pr/DlwrIlnAdOBPoQktbI73/3uFbD6CKCMi1wIZwHnhjsVLIhIDPA4MD3MooRaHUz3UB+eub46IdFXVfeEMymPDgEmq+piInA38V0S6qKov3IFFipp4R7AFaBGw3NxdV2YZEYnDuZ3cE5LovBHMNSMiFwB3AYNUNT9EsXmlomuuB3QBZovIRpy61KkR3mAczO85C5iqqgWqugFYjZMYIlUw1zwCeANAVb8BEnAGZ6upgvr/fjxqYiKYD7QVkZYiUgunMXhqqTJTgd+474cAn6vbChOhKrxmEekBPI+TBCK93hgquGZV3a+qKaqarqrpOO0ig1Q1kuc5DeZv+z2cuwFEJAWnqmh9CGOsasFc8yagH4CIdMRJBLtCGmVoTQV+7fYeOgvYr6rbKnPAGlc1pKqFInITMBOnx8HLqrpCRMYAC1R1KvASzu3jWpxGmavCF3HlBXnNjwJ1gTfddvFNqjoobEFXUpDXXKMEec0zgQtFJBMoAu5Q1Yi92w3ymm8HXhCRW3EajodH8hc7EZmMk8xT3HaP+4B4AFV9Dqcd5CJgLZAL/LbS54zgn5cxxpgqUBOrhowxxhwHSwTGGBPlLBEYY0yUs0RgjDFRzhKBMcZEOUsEploSkSIRWRLwSi+nbE4VnG+SiGxwz7XIfUL1eI/xooh0ct//X6ltX1c2Rvc4xT+X5SLygYg0qKB890gfjdN4z7qPmmpJRHJUtW5Vly3nGJOAaar6lohcCIxT1dMqcbxKx1TRcUXkP8BqVX2onPLDcUZdvamqYzE1h90RmIggInXdeRQWicj3InLUSKMicrKIzAn4xnyuu/5CEfnG3fdNEanoA3oO0Mbd9zb3WMtF5M/uuiQR+VBElrrrh7rrZ4tIhoj8E0h043jV3Zbj/vu6iFwcEPMkERkiIrEi8qiIzHfHmL8+iB/LN7iDjYnIGe41LhaRr0Wkvfsk7hhgqBvLUDf2l0XkO7dsWSO2mmgT7rG37WWvsl44T8UucV/v4jwFn+xuS8F5qrL4jjbH/fd24C73fSzOeEMpOB/sSe76vwL3lnG+ScAQ9/2vgG+BXsD3QBLOU9krgB7AFcALAfvWd/+djTvnQXFMAWWKY7wM+I/7vhbOKJKJwEjgbnd9bWAB0LKMOHMCru9NYIC7nAzEue8vAN523w8HngrY/2HgWvd9A5yxiJLC/fu2V3hfNW6ICVNjHFbV7sULIhIPPCwiPwd8ON+EmwLbA/aZD7zsln1PVZeIyHk4k5XMdYfWqIXzTbosj4rI3Tjj1IzAGb/mXVU95MbwDnAu8BHwmIiMxalO+vI4rmsG8C8RqQ0MAOao6mG3Ouo0ERnilquPM1jchlL7J4rIEvf6VwKfBJT/j4i0xRlmIf4Y578QGCQif3GXE4A091gmSlkiMJHiGiAV6KWqBeKMKJoQWEBV57iJ4mJgkog8DmQDn6jqsCDOcYeqvlW8ICL9yiqkqqvFmevgIuBBEflMVccEcxGqmicis4FfAENxJloBZ7apP6nqzAoOcVhVu4tIHZzxd24EnsSZgGeWql7mNqzPPsb+Alyhqj8EE6+JDtZGYCJFfWCnmwT6AkfNuSzOPMw7VPUF4EWc6f7mAb1FpLjOP0lE2gV5zi+BX4pIHRFJwqnW+VJETgFyVfV/OIP5lTVnbIF7Z1KWKTgDhRXfXYDzoT6qeB8Raeees0zqzDZ3M3C7/DSUevFQxMMDih7EqSIrNhP4k7i3R+KMSmuinCUCEyleBTJE5Hvg18CqMsr0AZaKyGKcb9v/UtVdOB+Mk0VkGU61UIdgTqiqi3DaDr7DaTN4UVUXA12B79wqmvuAB8vYfQKwrLixuJSPcSYG+lSd6RfBSVyZwCJxJi1/ngru2N1YluFMzPII8A/32gP3mwV0Km4sxrlziHdjW+Eumyhn3UeNMSbK2R2BMcZEOUsExhgT5SwRGGNMlLNEYIwxUc4SgTHGRDlLBMYYE+UsERhjTJT7f/nCl3o2bJk0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "plot_roc_curve(XGBClassifier_sos, X_test, y_test) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3], 'max_depth': [3, 4, 5, 6, 8, 10, 12, 15], 'min_child_weight': [1, 3, 5, 7], 'gamma': [0.0, 0.1, 0.2, 0.3, 0.4], 'colsample_bytree': [0.3, 0.4, 0.5, 0.7], 'eval_metric': ['aucpr'], 'objective': ['binary:hinge']}\n"
     ]
    }
   ],
   "source": [
    "## Hyper Parameter Optimization for XGBoost Classifier\n",
    "\n",
    "params_os={\n",
    " \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ],\n",
    " \"eval_metric\"      : [\"aucpr\"],\n",
    "\"objective\"      : [\"binary:hinge\"]\n",
    "#\"scale_pos_weight\"  : [1, 10, 25, 50, 75, 99, 100, 200]\n",
    "    \n",
    "}\n",
    "print (params_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter optimization using RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/xgboost-for-imbalanced-classification/\n",
    "XGBClassifier = xgb.XGBClassifier()\n",
    "random_searchCV=RandomizedSearchCV(XGBClassifier,param_distributions=params_os,n_iter=7,scoring='f1_macro',n_jobs=-1,#cv=4,\n",
    "                                 refit=True,cv=cv,\n",
    "                                 verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/xgboost-for-imbalanced-classification/\n",
    "#without CV optmization\n",
    "XGBClassifier = xgb.XGBClassifier()\n",
    "random_search=RandomizedSearchCV(XGBClassifier,param_distributions=params_os,n_iter=7,scoring='f1_macro',n_jobs=-1,#cv=4,\n",
    "                                 refit=True,cv=10,\n",
    "                                 verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "/home/mvisi/.local/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  70 | elapsed: 41.0min remaining:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed: 42.1min finished\n",
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100...\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=7, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5,\n",
       "                                                             0.7],\n",
       "                                        'eval_metric': ['aucpr'],\n",
       "                                        'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2,\n",
       "                                                          0.25, 0.3],\n",
       "                                        'max_depth': [3, 4, 5, 6, 8, 10, 12,\n",
       "                                                      15],\n",
       "                                        'min_child_weight': [1, 3, 5, 7],\n",
       "                                        'objective': ['binary:hinge']},\n",
       "                   scoring='f1_macro', verbose=3)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 7 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed: 32.4min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed: 103.7min finished\n",
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=10, random_state=1),\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weigh...\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=7, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5,\n",
       "                                                             0.7],\n",
       "                                        'eval_metric': ['aucpr'],\n",
       "                                        'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2,\n",
       "                                                          0.25, 0.3],\n",
       "                                        'max_depth': [3, 4, 5, 6, 8, 10, 12,\n",
       "                                                      15],\n",
       "                                        'min_child_weight': [1, 3, 5, 7],\n",
       "                                        'objective': ['binary:hinge']},\n",
       "                   scoring='f1_macro', verbose=3)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_searchCV.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
       "              gamma=0.0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=15, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.7, eval_metric='aucpr',\n",
       "              gamma=0.1, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
       "              max_depth=12, min_child_weight=5, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_searchCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier_RandomSearch = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
    "              gamma=0.0, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
    "              max_depth=15, min_child_weight=1,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
    "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier_RandomSearchCV = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.7, eval_metric='aucpr',\n",
    "              gamma=0.1, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
    "              max_depth=12, min_child_weight=5, \n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
    "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing which hyperparamter is causing the predict_proba as 1 \n",
    "XGBClassifier_RandomSearchCVtest = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.7, eval_metric='aucpr',\n",
    "              gamma=0.1, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
    "              max_depth=12, min_child_weight=5, \n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
    "              num_parallel_tree=1, objective='binary:logistic', \n",
    "                                                 random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
       "              gamma=0.0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=15, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifier_RandomSearch.fit(X_train_os, y_train_os) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.7, eval_metric='aucpr',\n",
       "              gamma=0.1, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
       "              max_depth=12, min_child_weight=5, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "              scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBClassifier_RandomSearchCV.fit(X_train_os, y_train_os) \n",
    "XGBClassifier_RandomSearchCVtest.fit(X_train_os, y_train_os) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124576    135]\n",
      " [   194     43]]\n",
      "0.997366904632327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    124711\n",
      "           1       0.24      0.18      0.21       237\n",
      "\n",
      "    accuracy                           1.00    124948\n",
      "   macro avg       0.62      0.59      0.60    124948\n",
      "weighted avg       1.00      1.00      1.00    124948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predXGB_RS=XGBClassifier_RandomSearch.predict(X_test) \n",
    "print(confusion_matrix(y_test,predXGB_RS)) \n",
    "print(accuracy_score(y_test,predXGB_RS)) \n",
    "print(classification_report(y_test,predXGB_RS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[123576   1135]\n",
      " [   145     92]]\n",
      "0.989755738387169\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    124711\n",
      "           1       0.07      0.39      0.13       237\n",
      "\n",
      "    accuracy                           0.99    124948\n",
      "   macro avg       0.54      0.69      0.56    124948\n",
      "weighted avg       1.00      0.99      0.99    124948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predXGB_RSCV=XGBClassifier_RandomSearchCVtest.predict(X_test) \n",
    "print(confusion_matrix(y_test,predXGB_RSCV)) \n",
    "print(accuracy_score(y_test,predXGB_RSCV)) \n",
    "print(classification_report(y_test,predXGB_RSCV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0(Approve as Legit)</th>\n",
       "      <th>Pred 1(Deny as Fraud)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0(Legit)</th>\n",
       "      <td>TN = 124679 (TNR = 99.97%)</td>\n",
       "      <td>FP = 32 (FPR = 0.03%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1(Fraud)</th>\n",
       "      <td>FN = 222 (FNR = 93.67%)</td>\n",
       "      <td>TP = 15 (TPR = 6.33%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Pred 0(Approve as Legit)  Pred 1(Deny as Fraud)\n",
       "True 0(Legit)  TN = 124679 (TNR = 99.97%)  FP = 32 (FPR = 0.03%)\n",
       "True 1(Fraud)     FN = 222 (FNR = 93.67%)  TP = 15 (TPR = 6.33%)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking with SMOTEK and threshold optimization\n",
    "from sklearn import model_selection, metrics\n",
    "hardpredtst=XGBClassifier_sos.predict(X_test)\n",
    "def conf_matrix(y,pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr,fpr),(fnr,tpr))= metrics.confusion_matrix(y, pred, \n",
    "            normalize='true')\n",
    "    return pd.DataFrame([[f'TN = {tn} (TNR = {tnr:1.2%})', \n",
    "                                f'FP = {fp} (FPR = {fpr:1.2%})'], \n",
    "                         [f'FN = {fn} (FNR = {fnr:1.2%})', \n",
    "                                f'TP = {tp} (TPR = {tpr:1.2%})']],\n",
    "            index=['True 0(Legit)', 'True 1(Fraud)'], \n",
    "            columns=['Pred 0(Approve as Legit)', \n",
    "                            'Pred 1(Deny as Fraud)'])\n",
    "conf_matrix(y_test,hardpredtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtstXGBC=XGBClassifier_sos.predict_proba(X_test)[:,1]\n",
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : Matrix with dependent or target data, where rows are observations\n",
    "\n",
    "    predicted : Matrix with predicted data, where rows are observations\n",
    "\n",
    "    Returns\n",
    "    -------     \n",
    "    list type, with optimal cutoff value\n",
    "\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(y_test, predtstXGBC)\n",
    "    i = np.arange(len(tpr)) \n",
    "    # tpr -(1-fpr) is zero near the optimal point\n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "    return list(roc_t['threshold']),roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtstXGBCHPCV=XGBClassifier_RandomSearchCVtest.predict_proba(X_test)[:,1]\n",
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : Matrix with dependent or target data, where rows are observations\n",
    "\n",
    "    predicted : Matrix with predicted data, where rows are observations\n",
    "\n",
    "    Returns\n",
    "    -------     \n",
    "    list type, with optimal cutoff value\n",
    "\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(y_test, predtstXGBCHPCV)\n",
    "    i = np.arange(len(tpr)) \n",
    "    # tpr -(1-fpr) is zero near the optimal point\n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "    return list(roc_t['threshold']),roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09858594089746475]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "Optimal_Cutoff,roc = Find_Optimal_Cutoff(y_test.values, predtstXGBCHPCV)\n",
    "print (Optimal_Cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0(Approve as Legit)</th>\n",
       "      <th>Pred 1(Deny as Fraud)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0(Legit)</th>\n",
       "      <td>TN = 93662 (TNR = 75.10%)</td>\n",
       "      <td>FP = 31049 (FPR = 24.90%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1(Fraud)</th>\n",
       "      <td>FN = 59 (FNR = 24.89%)</td>\n",
       "      <td>TP = 178 (TPR = 75.11%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Pred 0(Approve as Legit)      Pred 1(Deny as Fraud)\n",
       "True 0(Legit)  TN = 93662 (TNR = 75.10%)  FP = 31049 (FPR = 24.90%)\n",
       "True 1(Fraud)     FN = 59 (FNR = 24.89%)    TP = 178 (TPR = 75.11%)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardpredtst_tuned_threshXGBCHPCV = np.where(predtstXGBCHPCV >= 0.09858594089746475, 1, 0)\n",
    "conf_matrix(y_test, hardpredtst_tuned_threshXGBCHPCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0021155301947146654]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "Optimal_Cutoff,roc = Find_Optimal_Cutoff(y_test.values, predtstXGBC)\n",
    "print (Optimal_Cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predtstXGBCT=XGBClassifier_sos.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predtstXGBCT)\n",
    "dfplot=pd.DataFrame({'Threshold':thresholds, \n",
    "        'False Positive Rate':fpr, \n",
    "        'False Negative Rate': 1.-tpr})\n",
    "ax=dfplot.plot(x='Threshold', y=['False Positive Rate',\n",
    "        'False Negative Rate'], figsize=(10,6))\n",
    "#ax.plot([0.0022,0.0022],[0,0.3]) #mark example thresh.\n",
    "ax.set_xbound(0,0.01); ax.set_ybound(0,0.3) #zoom in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0(Approve as Legit)</th>\n",
       "      <th>Pred 1(Deny as Fraud)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0(Legit)</th>\n",
       "      <td>TN = 114148 (TNR = 91.53%)</td>\n",
       "      <td>FP = 10563 (FPR = 8.47%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1(Fraud)</th>\n",
       "      <td>FN = 101 (FNR = 42.62%)</td>\n",
       "      <td>TP = 136 (TPR = 57.38%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Pred 0(Approve as Legit)     Pred 1(Deny as Fraud)\n",
       "True 0(Legit)  TN = 114148 (TNR = 91.53%)  FP = 10563 (FPR = 8.47%)\n",
       "True 1(Fraud)     FN = 101 (FNR = 42.62%)   TP = 136 (TPR = 57.38%)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardpredtst_tuned_threshXGBCSOS = np.where(predtstXGBC >= 0.007600000000000007, 1, 0)\n",
    "conf_matrix(y_test, hardpredtst_tuned_threshXGBCSOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.007600000000000007\n",
    "\n",
    "0.0021155301947146654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predtstXGBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0(Approve as Legit)</th>\n",
       "      <th>Pred 1(Deny as Fraud)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0(Legit)</th>\n",
       "      <td>TN = 122879 (TNR = 98.53%)</td>\n",
       "      <td>FP = 1832 (FPR = 1.47%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1(Fraud)</th>\n",
       "      <td>FN = 127 (FNR = 53.59%)</td>\n",
       "      <td>TP = 110 (TPR = 46.41%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Pred 0(Approve as Legit)    Pred 1(Deny as Fraud)\n",
       "True 0(Legit)  TN = 122879 (TNR = 98.53%)  FP = 1832 (FPR = 1.47%)\n",
       "True 1(Fraud)     FN = 127 (FNR = 53.59%)  TP = 110 (TPR = 46.41%)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking with Randomoversampler and threshold optimization\n",
    "from sklearn import model_selection, metrics\n",
    "hardpredtstrs=XGBClassifier.predict(X_test)\n",
    "def conf_matrix(y,pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr,fpr),(fnr,tpr))= metrics.confusion_matrix(y, pred, \n",
    "            normalize='true')\n",
    "    return pd.DataFrame([[f'TN = {tn} (TNR = {tnr:1.2%})', \n",
    "                                f'FP = {fp} (FPR = {fpr:1.2%})'], \n",
    "                         [f'FN = {fn} (FNR = {fnr:1.2%})', \n",
    "                                f'TP = {tp} (TPR = {tpr:1.2%})']],\n",
    "            index=['True 0(Legit)', 'True 1(Fraud)'], \n",
    "            columns=['Pred 0(Approve as Legit)', \n",
    "                            'Pred 1(Deny as Fraud)'])\n",
    "conf_matrix(y_test,hardpredtstrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtstXGBCRS=XGBClassifier.predict_proba(X_test)[:,1]\n",
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : Matrix with dependent or target data, where rows are observations\n",
    "\n",
    "    predicted : Matrix with predicted data, where rows are observations\n",
    "\n",
    "    Returns\n",
    "    -------     \n",
    "    list type, with optimal cutoff value\n",
    "\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(y_test, predtstXGBCRS)\n",
    "    i = np.arange(len(tpr)) \n",
    "    # tpr -(1-fpr) is zero near the optimal point\n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "    return list(roc_t['threshold']),roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.043136004358530045]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "Optimal_Cutoff,roc = Find_Optimal_Cutoff(y_test.values, predtstXGBCRS)\n",
    "print (Optimal_Cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predtstXGBCTRS=XGBClassifier.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predtstXGBCTRS)\n",
    "dfplot=pd.DataFrame({'Threshold':thresholds, \n",
    "        'False Positive Rate':fpr, \n",
    "        'False Negative Rate': 1.-tpr})\n",
    "ax=dfplot.plot(x='Threshold', y=['False Positive Rate',\n",
    "        'False Negative Rate'], figsize=(10,6))\n",
    "#ax.plot([0.0022,0.0022],[0,0.3]) #mark example thresh.\n",
    "ax.set_xbound(0,0.18\n",
    "            ); ax.set_ybound(0,0.3) #zoom in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0(Approve as Legit)</th>\n",
       "      <th>Pred 1(Deny as Fraud)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0(Legit)</th>\n",
       "      <td>TN = 91714 (TNR = 73.54%)</td>\n",
       "      <td>FP = 32997 (FPR = 26.46%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1(Fraud)</th>\n",
       "      <td>FN = 63 (FNR = 26.58%)</td>\n",
       "      <td>TP = 174 (TPR = 73.42%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Pred 0(Approve as Legit)      Pred 1(Deny as Fraud)\n",
       "True 0(Legit)  TN = 91714 (TNR = 73.54%)  FP = 32997 (FPR = 26.46%)\n",
       "True 1(Fraud)     FN = 63 (FNR = 26.58%)    TP = 174 (TPR = 73.42%)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardpredtst_tuned_threshXGBCRS = np.where(predtstXGBCTRS >= 0.043136004358530045, 1, 0)\n",
    "conf_matrix(y_test, hardpredtst_tuned_threshXGBCRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0(Approve as Legit)</th>\n",
       "      <th>Pred 1(Deny as Fraud)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True 0(Legit)</th>\n",
       "      <td>TN = 57482 (TNR = 46.09%)</td>\n",
       "      <td>FP = 67229 (FPR = 53.91%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True 1(Fraud)</th>\n",
       "      <td>FN = 32 (FNR = 13.50%)</td>\n",
       "      <td>TP = 205 (TPR = 86.50%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Pred 0(Approve as Legit)      Pred 1(Deny as Fraud)\n",
       "True 0(Legit)  TN = 57482 (TNR = 46.09%)  FP = 67229 (FPR = 53.91%)\n",
       "True 1(Fraud)     FN = 32 (FNR = 13.50%)    TP = 205 (TPR = 86.50%)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.007600000000000007\n",
    "hardpredtst_tuned_threshXGBCRS = np.where(predtstXGBCTRS >= 0.007600000000000007, 1, 0)\n",
    "conf_matrix(y_test, hardpredtst_tuned_threshXGBCRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predXGB_RSCV=XGBClassifier_RandomSearchCV.predict(X_test) \n",
    "print(confusion_matrix(y_test,predXGB_RSCV)) \n",
    "print(accuracy_score(y_test,predXGB_RSCV)) \n",
    "print(classification_report(y_test,predXGB_RSCV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation and hyperparameter tunning is not giving good result compared to normal XGBoost without hyperparameter tunning related to Recall for Fraud but False positive have been reduced considerably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3], 'max_depth': [3, 4, 5, 6, 8, 10, 12, 15], 'min_child_weight': [1, 3, 5, 7], 'gamma': [0.0, 0.1, 0.2, 0.3, 0.4], 'colsample_bytree': [0.3, 0.4, 0.5, 0.7], 'eval_metric': ['aucpr'], 'objective': ['binary:hinge'], 'scale_pos_weight': [1, 10, 25, 50, 75, 99, 100, 200, 300, 400, 500, 600]}\n"
     ]
    }
   ],
   "source": [
    "params_os_posw={\n",
    " \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ],\n",
    " \"eval_metric\"      : [\"aucpr\"],\n",
    "\"objective\"      : [\"binary:hinge\"],\n",
    "\"scale_pos_weight\"  : [1, 10, 25, 50, 75, 99, 100, 200,300,400, 500,600]\n",
    "    \n",
    "}\n",
    "print (params_os_posw)\n",
    "#\"scale_pos_weight\"  : [1, 10, 25, 50, 75, 99, 100, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier = xgb.XGBClassifier()\n",
    "random_searchCVPOSW=RandomizedSearchCV(XGBClassifier,param_distributions=params_os_posw,n_iter=7,scoring='f1_macro',n_jobs=-1,#cv=4,\n",
    "                                 refit=True,cv=cv,\n",
    "                                 verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier = xgb.XGBClassifier()\n",
    "random_searchCVPOSW1=RandomizedSearchCV(XGBClassifier,param_distributions=params_os_posw,n_iter=7,scoring='f1_macro',n_jobs=-1,#cv=4,\n",
    "                                 refit=True,cv=cv,\n",
    "                                 verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 7 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed: 83.0min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed: 152.1min finished\n",
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:24:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=10, random_state=1),\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weigh...\n",
       "                   n_iter=7, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5,\n",
       "                                                             0.7],\n",
       "                                        'eval_metric': ['aucpr'],\n",
       "                                        'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2,\n",
       "                                                          0.25, 0.3],\n",
       "                                        'max_depth': [3, 4, 5, 6, 8, 10, 12,\n",
       "                                                      15],\n",
       "                                        'min_child_weight': [1, 3, 5, 7],\n",
       "                                        'objective': ['binary:hinge'],\n",
       "                                        'scale_pos_weight': [1, 10, 25, 50, 75,\n",
       "                                                             99, 100, 200, 300,\n",
       "                                                             400, 500, 600]},\n",
       "                   scoring='f1_macro', verbose=3)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Oversample data\n",
    "random_searchCVPOSW.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
       "              gamma=0.0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.3, max_delta_step=0,\n",
       "              max_depth=15, min_child_weight=3, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_searchCVPOSW.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier_RandomSearchCVPOSW = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
    "              gamma=0.0, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.3, max_delta_step=0,\n",
    "              max_depth=15, min_child_weight=3,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
    "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:57:47] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
       "              gamma=0.0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.3, max_delta_step=0,\n",
       "              max_depth=15, min_child_weight=3, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifier_RandomSearchCVPOSW.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124649     62]\n",
      " [   209     28]]\n",
      "0.9978310977366585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    124711\n",
      "           1       0.31      0.12      0.17       237\n",
      "\n",
      "    accuracy                           1.00    124948\n",
      "   macro avg       0.65      0.56      0.59    124948\n",
      "weighted avg       1.00      1.00      1.00    124948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predXGB_RS_POSW=XGBClassifier_RandomSearchCVPOSW.predict(X_test) \n",
    "print(confusion_matrix(y_test,predXGB_RS_POSW)) \n",
    "print(accuracy_score(y_test,predXGB_RS_POSW)) \n",
    "print(classification_report(y_test,predXGB_RS_POSW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale_pos_weight did not give good result in terms of Recall for positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 7 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed: 41.0min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed: 87.7min finished\n",
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:01:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=10, random_state=1),\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weigh...\n",
       "                   n_iter=7, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5,\n",
       "                                                             0.7],\n",
       "                                        'eval_metric': ['aucpr'],\n",
       "                                        'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2,\n",
       "                                                          0.25, 0.3],\n",
       "                                        'max_depth': [3, 4, 5, 6, 8, 10, 12,\n",
       "                                                      15],\n",
       "                                        'min_child_weight': [1, 3, 5, 7],\n",
       "                                        'objective': ['binary:hinge'],\n",
       "                                        'scale_pos_weight': [1, 10, 25, 50, 75,\n",
       "                                                             99, 100, 200, 300,\n",
       "                                                             400, 500, 600]},\n",
       "                   scoring='f1_macro', verbose=3)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using non-oversample data\n",
    "random_searchCVPOSW1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
       "              gamma=0.1, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.3, max_delta_step=0,\n",
       "              max_depth=8, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=400, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_searchCVPOSW1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier_RandomSearchCVPOSW1 = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
    "              gamma=0.1, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.3, max_delta_step=0,\n",
    "              max_depth=8, min_child_weight=1, \n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
    "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=400, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:22:52] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
       "              gamma=0.1, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.3, max_delta_step=0,\n",
       "              max_depth=8, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=400, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifier_RandomSearchCVPOSW1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124659     52]\n",
      " [   204     33]]\n",
      "0.9979511476774338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    124711\n",
      "           1       0.39      0.14      0.20       237\n",
      "\n",
      "    accuracy                           1.00    124948\n",
      "   macro avg       0.69      0.57      0.60    124948\n",
      "weighted avg       1.00      1.00      1.00    124948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predXGB_RS_POSW1=XGBClassifier_RandomSearchCVPOSW1.predict(X_test) \n",
    "print(confusion_matrix(y_test,predXGB_RS_POSW1)) \n",
    "print(accuracy_score(y_test,predXGB_RS_POSW1)) \n",
    "print(classification_report(y_test,predXGB_RS_POSW1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale_pos_weight did not give good result in terms of Recall for positive in non oversample data but better than oversample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3], 'max_depth': [3, 4, 5, 6, 8, 10, 12, 15], 'min_child_weight': [1, 3, 5, 7], 'gamma': [0.0, 0.1, 0.2, 0.3, 0.4], 'colsample_bytree': [0.3, 0.4, 0.5, 0.7], 'eval_metric': ['aucpr'], 'objective': ['binary:hinge'], 'max_delta_step': [1, 10, 30, 40, 50, 60, 70]}\n"
     ]
    }
   ],
   "source": [
    "params_os_mds={\n",
    " \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ],\n",
    " \"eval_metric\"      : [\"aucpr\"],\n",
    "\"objective\"      : [\"binary:hinge\"],\n",
    "\"max_delta_step\"  : [1, 10, 30, 40, 50, 60, 70]\n",
    "    \n",
    "}\n",
    "print (params_os_mds)\n",
    "#\"scale_pos_weight\"  : [1, 10, 25, 50, 75, 99, 100, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier = xgb.XGBClassifier()\n",
    "random_searchCVMDS=RandomizedSearchCV(XGBClassifier,param_distributions=params_os_mds,n_iter=7,scoring='f1_macro',n_jobs=-1,#cv=4,\n",
    "                                 refit=True,cv=cv,\n",
    "                                 verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier = xgb.XGBClassifier()\n",
    "random_searchCVMDS1=RandomizedSearchCV(XGBClassifier,param_distributions=params_os_mds,n_iter=7,scoring='f1_macro',n_jobs=-1,#cv=4,\n",
    "                                 refit=True,cv=cv,\n",
    "                                 verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 7 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed: 50.6min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed: 94.1min finished\n",
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=10, random_state=1),\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weigh...\n",
       "                                           verbosity=None),\n",
       "                   n_iter=7, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5,\n",
       "                                                             0.7],\n",
       "                                        'eval_metric': ['aucpr'],\n",
       "                                        'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2,\n",
       "                                                          0.25, 0.3],\n",
       "                                        'max_delta_step': [1, 10, 30, 40, 50,\n",
       "                                                           60, 70],\n",
       "                                        'max_depth': [3, 4, 5, 6, 8, 10, 12,\n",
       "                                                      15],\n",
       "                                        'min_child_weight': [1, 3, 5, 7],\n",
       "                                        'objective': ['binary:hinge']},\n",
       "                   scoring='f1_macro', verbose=3)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Oversample data\n",
    "random_searchCVMDS.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using normalsample data\n",
    "random_searchCVMDS1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_searchCVMDS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_searchCVMDS1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier_RandomSearchCVMDS = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
    "              gamma=0.0, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.3, max_delta_step=0,\n",
    "              max_depth=15, min_child_weight=3,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
    "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier_RandomSearchCVMDS1 = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
    "              gamma=0.0, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.3, max_delta_step=0,\n",
    "              max_depth=15, min_child_weight=3,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
    "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvisi/.local/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, eval_metric='aucpr',\n",
       "              gamma=0.1, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.1, max_delta_step=40,\n",
       "              max_depth=10, min_child_weight=5, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifier_RandomSearchCVMDS.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4, eval_metric='aucpr',\n",
       "              gamma=0.1, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.2, max_delta_step=60,\n",
       "              max_depth=15, min_child_weight=7, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='binary:hinge', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifier_RandomSearchCVMDS1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124033    678]\n",
      " [   162     75]]\n",
      "0.9932772033165796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    124711\n",
      "           1       0.10      0.32      0.15       237\n",
      "\n",
      "    accuracy                           0.99    124948\n",
      "   macro avg       0.55      0.66      0.57    124948\n",
      "weighted avg       1.00      0.99      1.00    124948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predXGB_RS_MDS=XGBClassifier_RandomSearchCVMDS.predict(X_test) \n",
    "print(confusion_matrix(y_test,predXGB_RS_MDS)) \n",
    "print(accuracy_score(y_test,predXGB_RS_MDS)) \n",
    "print(classification_report(y_test,predXGB_RS_MDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124690     21]\n",
      " [   210     27]]\n",
      "0.9981512309120594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    124711\n",
      "           1       0.56      0.11      0.19       237\n",
      "\n",
      "    accuracy                           1.00    124948\n",
      "   macro avg       0.78      0.56      0.59    124948\n",
      "weighted avg       1.00      1.00      1.00    124948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predXGB_RS_MDS1=XGBClassifier_RandomSearchCVMDS1.predict(X_test) \n",
    "print(confusion_matrix(y_test,predXGB_RS_MDS1)) \n",
    "print(accuracy_score(y_test,predXGB_RS_MDS1)) \n",
    "print(classification_report(y_test,predXGB_RS_MDS1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max delta step did not give good result in terms of Recall for positive in  normal data but better in oversample data. But non paramater hypertuned XGBoost have the best result till now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
